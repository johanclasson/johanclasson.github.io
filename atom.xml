<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Johan Classon Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.classon.eu/"/>
  <updated>2020-04-17T04:50:23.443Z</updated>
  <id>https://blog.classon.eu/</id>
  
  <author>
    <name>Johan Classon</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Release Notes With Azure DevOps</title>
    <link href="https://blog.classon.eu/2019-05-31-Release-Notes-with-Azure-DevOps/"/>
    <id>https://blog.classon.eu/2019-05-31-Release-Notes-with-Azure-DevOps/</id>
    <published>2019-05-30T22:00:00.000Z</published>
    <updated>2020-04-17T04:50:23.443Z</updated>
    
    <content type="html"><![CDATA[<p>The process that suites your team best might be different from what might be best for someone else. In this blog post, I would like to share how you can construct release notes with Azure DevOps in a nice ordered fashion.</p><p>When you need to publish release notes you might be facing difficulties such as:</p><ul><li>How to make them awesome (!).</li><li>How to reach your audience.</li><li>How to be as effective as possible.</li></ul><h2 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR;"></a>TL;DR;</h2><p>1)    Write release notes in Product Backlog Items (PBIs).<br>2)    Collect release notes from PBIs into a Markdown document.<br>3)    Review by pull request (PR).<br>4)    Publish.</p><h2 id="Making-Them-Awesome"><a href="#Making-Them-Awesome" class="headerlink" title="Making Them Awesome"></a>Making Them Awesome</h2><h3 id="What-Changes-Have-Been-Introduced-Since-Last-Release"><a href="#What-Changes-Have-Been-Introduced-Since-Last-Release" class="headerlink" title="What Changes Have Been Introduced Since Last Release?"></a>What Changes Have Been Introduced Since Last Release?</h3><p>Obviously, you need some way to find out which changes that have been made, and also, which of those that are candidates to be included in the release notes. </p><p>Just prior to the release, you could construct a list of all changes manually, for example by summarizing the pushed commits. This approach is optimal if your team works in an ad-hoc fashion or without planning.</p><p>If you really want to, you can diff what code that changed since last release, and then write the release notes with that as guidance.</p><p>You might be tempted to use a Git repository log as reference, be aware of that commit messages might not be the best source of information for release notes since they are often written for other developers! If developers are indeed your target audience, you could just as well direct them to your repository instead of summarizing its log in a document somewhere.</p><p>My point here is that I do not recommend you relying on a tool for summarizing your release notes. By just planning a little you will have your summary already. For example, if you decide beforehand what to include in a release, then you will already have a nice list of what to include in the release notes. </p><h3 id="What-Changes-Might-Be-of-Importance-to-the-Audience-and-What-Changes-Might-Not"><a href="#What-Changes-Might-Be-of-Importance-to-the-Audience-and-What-Changes-Might-Not" class="headerlink" title="What Changes Might Be of Importance to the Audience, and What Changes Might Not?"></a>What Changes Might Be of Importance to the Audience, and What Changes Might Not?</h3><p>In Azure DevOps I plan my releases with PBI work items. I have found it convenient to include a custom boolean field in the PBI that I can then use for making a query to select just the right PBIs to include in the release notes.</p><p>If you have different audiences, you can consider adding multiple boolean fields, one for each audience.</p><p>How to add custom fields to work items are described in the official <a href="https://docs.microsoft.com/en-us/azure/devops/organizations/settings/work/customize-process-field?view=azure-devops" target="_blank" rel="noopener">Azure DevOps documentation</a>.</p><h3 id="How-Can-You-Make-Sure-That-the-Information-in-the-Release-Notes-Are-Correct"><a href="#How-Can-You-Make-Sure-That-the-Information-in-the-Release-Notes-Are-Correct" class="headerlink" title="How Can You Make Sure That the Information in the Release Notes Are Correct?"></a>How Can You Make Sure That the Information in the Release Notes Are Correct?</h3><p>In my opinion, the ones best qualified to describe a change is probably the ones that have implemented it. This means that I do not think that it is a good idea to let just a few people write the release notes. Instead you should have your whole team engaged in writing them.</p><p>You might think that this is overkill, but in the best of worlds, I think that you would benefit from writing the release notes for a PBI before you start implementing it. The idea is to force you to think about the end-user or customer’s perspective as early as possible in your development process.</p><p>I recommend that you include custom text-fields for release notes directly in the PBI. That way it will be easy to find where to write. Here is an example.</p><p><img src="/2019-05-31-Release-Notes-with-Azure-DevOps/PBI-release-notes-fields.png" alt="PBI custom fields for making release notes with Azure DevOps"></p><p>If your team do not commit to writing release notes beforehand, you could add a reminder for everyone to finish their release notes by including it in your team’s definition of done.</p><h3 id="How-to-Make-the-Release-Notes-Readable"><a href="#How-to-Make-the-Release-Notes-Readable" class="headerlink" title="How to Make the Release Notes Readable?"></a>How to Make the Release Notes Readable?</h3><p>Of course you can make a best effort writing nice readable descriptions for each PBI. I imagine this is a matter of personal opinion, but I think it is hard to make proof-reeding when I only have the release notes from a single PBI on the screen. Therefore, I like to collect all release notes from the PBIs into a single document before the review process begins.</p><p>I treat that single document as the final product, and hence I often do not care to update the descriptions in the PBIs. These can be seen upon as just history if you will.</p><p>You can find an example how to generate release notes in Markdown format using PowerShell in <a href="https://gist.github.com/johanclasson/027f49ef0bf9e924927554b4926266bd" target="_blank" rel="noopener">this gist</a>. The script iterates all work items returned by a query and constructs the document content from the custom title- and description-fields in those.</p><p>Here is an example of how to use it:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$content</span> = .\<span class="built_in">Get-ReleaseNotes</span>.ps1 <span class="literal">-Pat</span> <span class="string">'abc123'</span> <span class="literal">-Organization</span> <span class="string">'Fabrikam'</span> <span class="literal">-Project</span> <span class="string">'Fiber'</span> <span class="literal">-Query</span> <span class="string">'My Queries/Release Notes'</span></span><br><span class="line"><span class="variable">$utf8NoBomEncoding</span> = <span class="built_in">New-Object</span> System.Text.UTF8Encoding <span class="variable">$False</span></span><br><span class="line"><span class="variable">$path</span> = <span class="built_in">Join-Path</span> (<span class="built_in">Resolve-Path</span> .\notes) sprint<span class="literal">-11</span>.md</span><br><span class="line">[<span class="type">System.IO.File</span>]::WriteAllLines(<span class="variable">$path</span>, <span class="variable">$content</span>, <span class="variable">$utf8NoBomEncoding</span>) <span class="comment"># Outputs UTF8 without BOM</span></span><br></pre></td></tr></table></figure><p>My format of choice for release notes is Markdown, and that is because of the following reasons.</p><ul><li>Markdown is a perfect format for making reviews through PRs to a Git repository.</li><li>It is relatively easy to convert Markdown into whatever format you need to publish in.</li></ul><p>I would like to stress that the more people that are engaged in both writing and reviewing release notes the better they become. In addition to involving technical experts in the review process, I also like to have generally skilled writers review the document at least once before it is published.</p><h2 id="How-to-Reach-Your-Audience"><a href="#How-to-Reach-Your-Audience" class="headerlink" title="How to Reach Your Audience"></a>How to Reach Your Audience</h2><p>Difference audiences a best reached by different means. For example, if you do not have direct contact with your end users you might want to publish your release notes as HTML on your product’s website. Or if your audience does not actively search for release notes you might want to consider publishing your release notes as a PDF-file which you mail to your end users.</p><p>If you would like to be fancy, you can integrate the release notes into the actual product so that your end users can pull up-, or are presented with-, the release notes just as they use the new version of your product the first time.</p><h2 id="How-to-Be-as-Effective-as-Possible"><a href="#How-to-Be-as-Effective-as-Possible" class="headerlink" title="How to Be as Effective as Possible"></a>How to Be as Effective as Possible</h2><p>Doing all of this planning, PBI-tinkering and reviewing might seem like a lot of work. But I hope that you might soon realize that it is indeed less work compared to manually construct release notes in an ad-hoc fashion.</p><p>Also, when you write the release notes close to when you design or implement a change, then you do not have to spend as much time figuring out what to write.</p><p>I think that being effective is not only about spending less time writing release notes, but also that you also produce content with high quality. The review process is essential to archive this. Please do not skip reviewing of your release notes!</p><p>If your teams are already familiar with doing PRs to review code, reviewing release notes in the same way should not be so complicated.</p><h2 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h2><p>Here I will summarize an example of how the overall process might look like. </p><p>The release notes owner manages a work item query which selects the work items which are meant to be included in the release notes.</p><p>An overview of the process around creating release notes is presented below.</p><p><img src="/2019-05-31-Release-Notes-with-Azure-DevOps/process-overview.png" alt="Process Overview"></p><h3 id="Two-Weeks-Prior-Release"><a href="#Two-Weeks-Prior-Release" class="headerlink" title="Two Weeks Prior Release"></a>Two Weeks Prior Release</h3><p>The release notes owner executes the query and mails its result to the team members who can see if some work items are missing or should be removed.</p><h3 id="One-Week-Prior-Release"><a href="#One-Week-Prior-Release" class="headerlink" title="One Week Prior Release"></a>One Week Prior Release</h3><p>When everyone has completed entering the release notes in the selected work items, the release notes owner generates a Markdown-file and starts a pull request. The team reviews the document, correcting mistakes until everyone agrees that the quality is high enough at which point the pull request is merged.</p><h3 id="Day-of-Release"><a href="#Day-of-Release" class="headerlink" title="Day of Release"></a>Day of Release</h3><p>The release notes owner triggers an automated process which converts the Markdown-file to various possible formats.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The process that suites your team best might be different from what might be best for someone else. In this blog post, I would like to sh
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Asynchronously Wait for Task to Complete With Timeout</title>
    <link href="https://blog.classon.eu/2018-11-29-Asynchronously-Wait-for-Task-to-Complete-with-Timeout/"/>
    <id>https://blog.classon.eu/2018-11-29-Asynchronously-Wait-for-Task-to-Complete-with-Timeout/</id>
    <published>2018-11-28T23:00:00.000Z</published>
    <updated>2020-04-17T07:15:50.809Z</updated>
    
    <content type="html"><![CDATA[<p>I was recently working at an async method which could possibly hang if some dependent stuff did not happen. To prevent the method from hanging, I wanted to implement some kind of timeout. Now, how can I make a task abort, given the following method signature?</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">SomeClient</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">async</span> Task <span class="title">DoStuffAsync</span>(<span class="params">CancellationToken? ct = <span class="literal">null</span></span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Take-One-Task-Extension-Method"><a href="#Take-One-Task-Extension-Method" class="headerlink" title="Take One - Task Extension Method"></a>Take One - Task Extension Method</h2><p>The plan was to implement som kind of extension method to Task which could add the timeout-functionality.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">internal</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">TaskExtensions</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">async</span> Task <span class="title">TimeoutAfter</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">this</span> Task task, <span class="keyword">int</span> millisecondsTimeout, CancellationToken ct</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">var</span> completedTask = <span class="keyword">await</span> Task.WhenAny(</span><br><span class="line">            task, Task.Delay(millisecondsTimeout, ct));</span><br><span class="line">        <span class="keyword">if</span> (completedTask == task)</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>And use it like this.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">SomeClient</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">async</span> Task <span class="title">DoStuffAsync</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        CancellationToken? ct = <span class="literal">null</span>, <span class="keyword">int</span> millisecondsTimeout = <span class="number">20</span>_0000</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">var</span> notNullCt = ct ?? CancellationToken.None;</span><br><span class="line">        <span class="keyword">await</span> DoStuffInnerAsync(notNullCt).TimeoutAfter(</span><br><span class="line">            millisecondsTimeout, notNullCt);</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">async</span> Task <span class="title">DoStuffInnerAsync</span>(<span class="params">CancellationToken ct</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This design allowed that the internal <code>Delay</code>-task would be cancelled if the user of my API canceled the method call. Nice! But it also had some mayor disadvantages:</p><ul><li>No task will be canceled if either of them finish successfully, which leads to having tasks running in the background for no reason, eating system resources.</li><li>I had to make sure to pass the same cancellation token both to <code>DoStuffInnerAsync</code> and <code>TimeoutAfter</code>, which might be something that could lead to mistakes further down.</li></ul><h2 id="Take-Two-Expanding-the-Extension-Method"><a href="#Take-Two-Expanding-the-Extension-Method" class="headerlink" title="Take Two - Expanding the Extension Method"></a>Take Two - Expanding the Extension Method</h2><p>To be able to cancel the <code>TimeoutAfter</code>-task i needed a <code>CancellationTokenSource</code>-instance, and pass its token to the <code>TimeoutAfter</code>-method. And I also wanted the <code>TimeoutAfter</code>-task to cancel if the user canceled the public API call.</p><p>This is what I came up with.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">internal</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">TaskExtensions</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">async</span> Task <span class="title">TimeoutAfter</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">this</span> Task task, <span class="keyword">int</span> millisecondsTimeout, CancellationToken ct</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">using</span> (<span class="keyword">var</span> cts = <span class="keyword">new</span> CancellationTokenSource())</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">using</span> (ct.Register(() =&gt; cts.Cancel()))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">var</span> completedTask = <span class="keyword">await</span> Task.WhenAny(</span><br><span class="line">                    task, Task.Delay(millisecondsTimeout, cts.Token));</span><br><span class="line">                <span class="keyword">if</span> (completedTask == task)</span><br><span class="line">                &#123;</span><br><span class="line">                    cts.Cancel();</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException();                    </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This is some seriously dangerous programming.</p><ul><li>By subscribing to cancel-events with <code>ct.Register(...)</code> I opened upp the possibility for memory leaks if I do not unsubscribe somehow.</li><li>Also, using <code>cts</code> (which can be disposed) in the delegate passed to <code>ct.Register(...)</code> might actually make my application crash if <code>ct</code> was canceled outside of the <code>TimeOutAfter</code>-method scope.</li></ul><p><code>Register</code> returns a disposable something, which when disposed will unsubscribe. By adding the inner using-block, I fixed both of these problems.</p><p>This made it possible to cancel the <code>Delay</code>-task when the actual task completed, but not the reverse. How should I solve the bigger problem, how to cancel the actual task if it would hang? Eating up system resources indefinitely…</p><h2 id="Take-Three-Changing-the-Public-API"><a href="#Take-Three-Changing-the-Public-API" class="headerlink" title="Take Three - Changing the Public API"></a>Take Three - Changing the Public API</h2><p>With much hesitation I finally decided to make a breaking change to the public API by replacing the <code>CancellationToken</code> with a <code>CancellationTokenSource</code> in the <code>DoStuffAsync</code>-method.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">SomeClient</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">async</span> Task <span class="title">DoStuffAsync</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        CancellationTokenSource cts = <span class="literal">null</span>, <span class="keyword">int</span> millisecondsTimeout = <span class="number">20</span>_0000</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">var</span> notNullCts = cts ?? <span class="keyword">new</span> CancellationTokenSource();</span><br><span class="line">        <span class="keyword">await</span> DoStuffInnerAsync(notNullCts.Token).TimeoutAfter(</span><br><span class="line">            millisecondsTimeout, notNullCts);</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">async</span> Task <span class="title">DoStuffInnerAsync</span>(<span class="params">CancellationToken ct</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">internal</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">TaskExtensions</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">async</span> Task <span class="title">TimeoutAfter</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="keyword">this</span> Task task, <span class="keyword">int</span> millisecondsTimeout, CancellationTokenSource cts</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">var</span> completedTask = <span class="keyword">await</span> Task.WhenAny(</span><br><span class="line">            task, Task.Delay(millisecondsTimeout, cts.Token));</span><br><span class="line">        <span class="keyword">if</span> (completedTask == task)</span><br><span class="line">        &#123;</span><br><span class="line">            cts.Cancel();</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cts.Cancel();</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException();                    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Nice! But this still did not solve that I had make sure to pass the same <code>cts</code> to both the actual task and the <code>Delay</code>-task.</p><h2 id="Final-Solution-Doing-the-Obvious"><a href="#Final-Solution-Doing-the-Obvious" class="headerlink" title="Final Solution - Doing the Obvious"></a>Final Solution - Doing the Obvious</h2><p>Most things is really easy when you know the answer. By accident I stumbled upon that <code>CancellationTokenSource</code> has a <code>CancelAfter(...)</code>-method. This solves my problem entirely without the need to update my public API.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> client = <span class="keyword">new</span> SomeClient();</span><br><span class="line"><span class="keyword">var</span> cts = <span class="keyword">new</span> CancellationTokenSource();</span><br><span class="line">cts.CancelAfter(<span class="number">20</span>_000);</span><br><span class="line"><span class="keyword">await</span> client.DoStuffAsync(cts.Token);</span><br></pre></td></tr></table></figure><p>Easy peasy. I wish I had known about this earlier!</p>]]></content>
    
    <summary type="html">
    
      Most things is really easy when you know the answer. This is a story of how i struggled, making a complicated solution to a simple problem.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Get Started With Invisible reCAPTCHA</title>
    <link href="https://blog.classon.eu/2018-06-29-Get-started-with-invisible-reCAPTCHA/"/>
    <id>https://blog.classon.eu/2018-06-29-Get-started-with-invisible-reCAPTCHA/</id>
    <published>2018-06-28T22:00:00.000Z</published>
    <updated>2020-04-17T07:00:29.975Z</updated>
    
    <content type="html"><![CDATA[<p>To prevent software from posting forms on websites (and nowadays even on mobile apps), there is this <strong>C</strong>ompletely <strong>A</strong>utomated <strong>P</strong>ublic <strong>T</strong>uring test to tell <strong>C</strong>omputers and <strong>H</strong>umans <strong>A</strong>part, also known as CAPTCHA.</p><p>When I think about CAPTCHAs, I relate to trying to read distorted letters. I don?t know how you react, but I really dislike when I am forced to solve those. Lately I have seen a few variants such as differentiating cats from dogs, picking out road signs or other objects in more or less obscure images. Although I find them to be much more pleasant, I still think that they are a PITA.</p><p><a href="https://security.googleblog.com/2014/12/are-you-robot-introducing-no-captcha.html" target="_blank" rel="noopener">Late in 2014</a> Google announced their No CAPTCHA reCAPTCHA service using machine learning algorithms to detect whether a visitor is a bot or an actual person. While being much easier to use, I think it is kind of silly to have to tick a checkbox where I claim not to be a robot.</p><p>–</p><p>I must have been living under a rock the past year. I just recently found out that Google have an invisible version of their reCAPTCHA service, which was announced as early as <a href="https://security.googleblog.com/2017/06/making-internet-safer-and-faster.html" target="_blank" rel="noopener">march 2017</a>!</p><p>To get started:</p><p>1) <a href="https://www.google.com/recaptcha/admin" target="_blank" rel="noopener">Register for an API key</a></p><p>2) Include something similar to the following in your html</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://www.google.com/recaptcha/api.js"</span> <span class="attr">async</span> <span class="attr">defer</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="actionscript">    <span class="function"><span class="keyword">function</span> <span class="title">onSubmit</span><span class="params">(token)</span> </span>&#123;</span></span><br><span class="line"><span class="javascript">        <span class="built_in">document</span>.getElementById(<span class="string">"login-form"</span>).submit();</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">...</span><br><span class="line"><span class="tag">&lt;<span class="name">form</span> <span class="attr">id</span>=<span class="string">"login-form"</span> <span class="attr">...</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">"g-recaptcha"</span></span></span><br><span class="line"><span class="tag">            <span class="attr">data-sitekey</span>=<span class="string">"&#123;Insert API key here&#125;"</span></span></span><br><span class="line"><span class="tag">            <span class="attr">data-callback</span>=<span class="string">"onSubmit"</span>&gt;</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3) Validate the posted form parameter <code>g-recaptcha-response</code> service side by making a HTTP POST to <code>https://www.google.com/recaptcha/api/siteverify</code> with the query parameters <code>?secret={API secret}&amp;response={g-recaptcha-response}&amp;remoteip={Remote user IP}</code>.</p><p>The invisible reCAPTCHA is in my opinion a truly incredible service! Read more about it in <a href="https://developers.google.com/recaptcha/docs/invisible" target="_blank" rel="noopener">the official docs</a>, or <a href="https://www.google.com/recaptcha/api2/demo?invisible=true" target="_blank" rel="noopener">try the demo</a>.</p>]]></content>
    
    <summary type="html">
    
      This post explains how to get started with Googles invisible reCAPTCHA service.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Handle Secrets With .NET Core</title>
    <link href="https://blog.classon.eu/2018-05-09-Handle-Secrets-with-.NET-Core/"/>
    <id>https://blog.classon.eu/2018-05-09-Handle-Secrets-with-.NET-Core/</id>
    <published>2018-05-08T22:00:00.000Z</published>
    <updated>2020-04-17T07:00:29.972Z</updated>
    
    <content type="html"><![CDATA[<p>Have you ever checked in a configuration file with a secret? Now is the time for you to do something better. In this post, I will show four convenient ways in which configuration secrets can be stored outside of your source code repository.</p><h4 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h4><h5 id="NuGet-package-Microsoft-Extensions-Configuration-EnvironmentVariables"><a href="#NuGet-package-Microsoft-Extensions-Configuration-EnvironmentVariables" class="headerlink" title="NuGet-package Microsoft.Extensions.Configuration.EnvironmentVariables:"></a>NuGet-package Microsoft.Extensions.Configuration.EnvironmentVariables:</h5><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">string</span> secret = builder</span><br><span class="line">    .AddEnvironmentVariables(prefix)</span><br><span class="line">    .Build()[key];</span><br></pre></td></tr></table></figure><h5 id="NuGet-package-Microsoft-Extensions-Configuration-UserSecrets"><a href="#NuGet-package-Microsoft-Extensions-Configuration-UserSecrets" class="headerlink" title="NuGet-package Microsoft.Extensions.Configuration.UserSecrets:"></a>NuGet-package Microsoft.Extensions.Configuration.UserSecrets:</h5><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">string</span> secret = builder</span><br><span class="line">    .AddUserSecrets(userSecretsId)</span><br><span class="line">    .Build()[key];</span><br></pre></td></tr></table></figure><p>File with secrets at <code>%APPDATA%\Microsoft\UserSecrets\{userSecretsId}\secrets.json</code>.</p><h5 id="NuGet-package-Microsoft-Extensions-Configuration-AzureKeyVault"><a href="#NuGet-package-Microsoft-Extensions-Configuration-AzureKeyVault" class="headerlink" title="NuGet-package Microsoft.Extensions.Configuration.AzureKeyVault:"></a>NuGet-package Microsoft.Extensions.Configuration.AzureKeyVault:</h5><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">string</span> secret = builder</span><br><span class="line">    .AddAzureKeyVault(vaultUrl, clientId, certificate)</span><br><span class="line">    .Build()[key];</span><br></pre></td></tr></table></figure><h5 id="NuGet-packages-Microsoft-Azure-KeyVault-and-Microsoft-Azure-Services-AppAuthentication"><a href="#NuGet-packages-Microsoft-Azure-KeyVault-and-Microsoft-Azure-Services-AppAuthentication" class="headerlink" title="NuGet-packages Microsoft.Azure.KeyVault and Microsoft.Azure.Services.AppAuthentication:"></a>NuGet-packages Microsoft.Azure.KeyVault and Microsoft.Azure.Services.AppAuthentication:</h5><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> secret = <span class="keyword">await</span> <span class="keyword">new</span> KeyVaultClient(<span class="keyword">new</span> KeyVaultClient.AuthenticationCallback(</span><br><span class="line">    <span class="keyword">new</span> AzureServiceTokenProvider().KeyVaultTokenCallback))</span><br><span class="line">    .GetSecretAsync(vaultUrl, key);</span><br></pre></td></tr></table></figure><h2 id="Secrets-Stored-Locally"><a href="#Secrets-Stored-Locally" class="headerlink" title="Secrets Stored Locally"></a>Secrets Stored Locally</h2><p>Imagine that you are part of a team and that you work on developing some new system. Since the system depends on some back-end service you have a password in the systems .config- or .json configuration file, if not for something else, just so that you can to run the system on your local machine.</p><p>When you commit your code, you couple the access restrictions of the beck-end service to that of the repository. Or in other words, you will have to make sure that no one has access to the source code that is not allowed to access the back-end service.</p><p>Yes, you can always remove the password from the repository history, or even rotate the secret on the back-end service when development on your new system has stopped. But those two options might show to be quite cumbersome in practice.</p><p>A better option would have been to store the password on a location outside of your source code repository.</p><h3 id="Environment-Variables"><a href="#Environment-Variables" class="headerlink" title="Environment Variables"></a>Environment Variables</h3><p>Using environment variables to store secrets is a simple but effective solution. Since environment variables is a standardized mechanism that is available in <a href="https://en.wikipedia.org/wiki/Environment_variable" target="_blank" rel="noopener">most</a> operating systems, there is a wide range of tooling that can make your life easier. One example is Docker which has great support for setting environment variables for containers.</p><p>Here is an example of how to use the <code>AddEnvironmentVariables</code> extension method of the NuGet package <code>Microsoft.Extensions.Configuration.EnvironmentVariables</code>.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">var</span> builder = <span class="keyword">new</span> ConfigurationBuilder();</span><br><span class="line">        builder.AddEnvironmentVariables(<span class="string">"MYAPP_"</span>);</span><br><span class="line">        IConfiguration configuration = builder.Build();</span><br><span class="line">        Console.WriteLine(<span class="string">$"Ex1 EnvironmentVariables: <span class="subst">&#123;configuration[<span class="string">"MySecret"</span>]&#125;</span>"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In my opinion, it is a good practice to use a prefix filter. In the myriad of other environment variables, I find it nice to have the ones belonging to my apps grouped together. In my example above, the value of the variable with name <code>MYAPP_MYSECRET</code> will be available through <code>configuration[&quot;MySecret&quot;]</code>.</p><p>Also, a good feature is to group related variables into sections. When environment variables are added to the configuration instance, names with <code>__</code> are replaced with <code>:</code>. This enables <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/options?#suboptions-configuration" target="_blank" rel="noopener">getting sub sections, or/and using the options functionality</a>. </p><p>If you are used to get configuration values through a static resource such as the <code>ConfigurationManager</code> of the .NET ?Full? Framework, I recommend reading about <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection" target="_blank" rel="noopener">how dependency injection can be done with ASP.NET Core</a>, just to get you started with how to inject the configuration instance to where it is needed.</p><h3 id="User-Secrets"><a href="#User-Secrets" class="headerlink" title="User Secrets"></a>User Secrets</h3><p>Another option is to use the <code>AddUserSecrets</code> extension method of the NuGet package <code>Microsoft.Extensions.Configuration.UserSecrets</code>. It enforces that the secrets are stored in the AppData- or Home-directory.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">var</span> builder = <span class="keyword">new</span> ConfigurationBuilder();</span><br><span class="line">        <span class="keyword">var</span> env = <span class="keyword">new</span> HostingEnvironment &#123; EnvironmentName = EnvironmentName.Development &#125;;</span><br><span class="line">        <span class="keyword">if</span> (env.IsDevelopment())</span><br><span class="line">        &#123;</span><br><span class="line">            builder.AddUserSecrets(<span class="string">"MyUserSecretsId"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        IConfiguration configuration = builder.Build();</span><br><span class="line">        Console.WriteLine(<span class="string">$"Ex2 UserSecrets: <span class="subst">&#123;configuration[<span class="string">"MySecret"</span>]&#125;</span>"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Secrets are read from a json-file located at <code>%APPDATA%\Microsoft\UserSecrets\{UserSecretsId}\secrets.json</code> on Windows or <code>~/.microsoft/usersecrets/{UserSecretsId}/secrets.json</code> on Linux. I find it most convenient to supply the application id directly in the <code>AddUserSecrets</code>-method, but you can <a href="https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?#installing-the-secret-manager-tool" target="_blank" rel="noopener">set the id in the .csproj-file</a> as well, or by using the assembly attribute like <code>[assembly:UserSecretsId(&quot;MyUserSecrets&quot;)]</code>. If you really want to do something hard core, you can set the UserSecretsId with a MSBuild property like <code>/p:UserSecretsId=MyUserSecrets</code>.</p><p>There is some <a href="https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?#installing-the-secret-manager-tool" target="_blank" rel="noopener">CLI tooling</a> ment help you manage the secrets.json file. But I find it so dead simple to create the secrets.json file manually that I seldom bother to use the CLI.</p><p>The content of secrests.json can be something like this:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"MySecret"</span>: <span class="string">"Some value"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Since it is not a good practice to have unencrypted secrets on the file system, I recommend only using <code>UserSecrets</code> for local development.</p><h2 id="Secrets-Stored-in-a-Shared-Location"><a href="#Secrets-Stored-in-a-Shared-Location" class="headerlink" title="Secrets Stored in a Shared Location"></a>Secrets Stored in a Shared Location</h2><p>Back to my imaginary development scenario.</p><p>All is working well, and every team member is productive running the system. Your team is making great progress. But then all of a sudden, the back-end service password is rotated for whatever reason, and productivity stops.</p><p>Of course, one could prioritize to take the time to prepare everyone in the team of the rotation. But if you don?t, your colleagues will get runtime exceptions when they run their system. When they have found out that the reason that they got access denied was because of a rotated password, they can finally get up to speed again. That is, if they are able to find out what the new password is.</p><p>A possible way to solve this is to store the password in a shared location such in a common database, which preferably is encrypted. That is a solution that works well, and it is normally not that hard to encrypt the passwords. At least not with SQL Server.</p><h3 id="Azure-Key-Vault"><a href="#Azure-Key-Vault" class="headerlink" title="Azure Key Vault"></a>Azure Key Vault</h3><p>An even better approach would be to use a product specialized for secrets distribution, such as Azure Key Vault (AKV). I think the tooling around AKV is great. Since it has a REST API, you can access it from most platforms, and there <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-keyvault-parameter" target="_blank" rel="noopener">is support in Azure Arm Templates</a> to get secrets from AKV when they are run. Besides usability, AKV is <a href="https://azure.microsoft.com/en-us/pricing/details/key-vault/" target="_blank" rel="noopener">not very expensive</a> to use for secrets. It is so cheap that I mostly consider it to be a free service.</p><p>For accessing secrets in AKV one needs to authenticate and pass in an access token. Applications that need to authenticate with Azure Active Directory (AAD) do so with credentials stored in service principals. If needed, an application can have many ways to authenticate, and each set of credentials is stored in a separate service principal.</p><p>Authentication with service principals are made with an application id and either a client secret or a certificate. So, which one is the better option?</p><p>Secrets might seem easy to use but have the drawback that they can be read. By using secrets, you again couple the access restrictions. This time, the coupling is between access to the settings of the application and to whatever resources that application is meant to access.</p><p>Certificates are in fact also relatively straight-forward to use. You can even generate a self-signed certificate and use that to create a service principal in AAD with just these few lines of PowerShell:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cert</span> = <span class="built_in">New-SelfSignedCertificate</span> <span class="literal">-CertStoreLocation</span> <span class="string">"cert:\CurrentUser\My"</span> `</span><br><span class="line">  <span class="literal">-Subject</span> <span class="string">"CN=my-application"</span> <span class="literal">-KeySpec</span> KeyExchange</span><br><span class="line"><span class="variable">$keyValue</span> = [<span class="type">System.Convert</span>]::ToBase64String(<span class="variable">$cert</span>.GetRawCertData())</span><br><span class="line"></span><br><span class="line"><span class="variable">$sp</span> = <span class="built_in">New-AzureRMADServicePrincipal</span> <span class="literal">-DisplayName</span> my<span class="literal">-application</span> `</span><br><span class="line">  <span class="literal">-CertValue</span> <span class="variable">$keyValue</span> <span class="literal">-EndDate</span> <span class="variable">$cert</span>.NotAfter <span class="literal">-StartDate</span> <span class="variable">$cert</span>.NotBefore</span><br><span class="line">Sleep <span class="number">20</span> <span class="comment"># Wait for service principal to be propagated throughout AAD</span></span><br><span class="line"><span class="built_in">New-AzureRmRoleAssignment</span> <span class="literal">-RoleDefinitionName</span> Contributor `</span><br><span class="line">  <span class="literal">-ServicePrincipalName</span> <span class="variable">$sp</span>.ApplicationId</span><br></pre></td></tr></table></figure><p>If you want a longer certificate validity than one year, use the argument <code>-NotAfter</code> for <code>New-SelfSignedCertificate</code>.</p><p>Here is an example of how to get a secret from AKV by using the <code>AddAzureKeyVault</code> extension method of the NuGet package <code>Microsoft.Extensions.Configuration.AzureKeyVault</code>:</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">Program</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">Main</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">var</span> builder = <span class="keyword">new</span> ConfigurationBuilder();</span><br><span class="line">        builder.AddAzureKeyVault(</span><br><span class="line">            vault: <span class="string">"https://my-application-kv.vault.azure.net/"</span>,</span><br><span class="line">            clientId: <span class="string">"865f36f7-08c1-4ca2-97c9-a5a9cab56fd8"</span>,</span><br><span class="line">            certificate: GetCertificate());</span><br><span class="line">        IConfiguration configuration = builder.Build();</span><br><span class="line">        Console.WriteLine(<span class="string">$"Ex3 AzureKeyVault: <span class="subst">&#123;configuration[<span class="string">"MySecret"</span>]&#125;</span>"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> X509Certificate2 <span class="title">GetCertificate</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">using</span> (X509Store store = <span class="keyword">new</span> X509Store(StoreLocation.CurrentUser))</span><br><span class="line">        &#123;</span><br><span class="line">            store.Open(OpenFlags.ReadOnly);</span><br><span class="line">            <span class="keyword">var</span> cers = store.Certificates.Find(</span><br><span class="line">                X509FindType.FindBySubjectName, <span class="string">"my-application"</span>, <span class="literal">false</span>);</span><br><span class="line">            <span class="keyword">if</span> (cers.Count == <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"Could not find certificate!"</span>);</span><br><span class="line">            <span class="keyword">return</span> cers[<span class="number">0</span>];                </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For loading a certificate in an Azure App Service, add a <code>WEBSITE_LOAD_CERTIFICATES</code> app setting with the certificate thumbprint as value. For more details on this, read the <a href="https://docs.microsoft.com/en-us/azure/app-service/app-service-web-ssl-cert-load#load-your-certificates" target="_blank" rel="noopener">official docs</a>.</p><p>When calling the <code>Build</code>-method, all secrets are read in from AKV at once and are then kept in the configuration. Secret names with <code>--</code> are replaced with <code>:</code> when they are read in.</p><h3 id="AppAuthentication-and-KeyVaultClient"><a href="#AppAuthentication-and-KeyVaultClient" class="headerlink" title="AppAuthentication and KeyVaultClient"></a>AppAuthentication and KeyVaultClient</h3><p>Just a few days ago, Microsoft released the NuGet package <code>Microsoft.Azure.Services.AppAuthentication</code>. It contains an <code>AzureServiceTokenProvider</code> that abstracts how to get an access token from AAD.<br>To use it together with the <code>KeyVaultClient</code> in the NuGet package <code>Microsoft.Azure.KeyVault</code>, you simply insert a callback method of the token provider in its constructor.</p><pre><code>public static class Program{    private static async Task Main()    {        var azureServiceTokenProvider = new AzureServiceTokenProvider();        var keyVaultClient = new KeyVaultClient(            new KeyVaultClient.AuthenticationCallback(                azureServiceTokenProvider.KeyVaultTokenCallback));        var secret = await keyVaultClient            .GetSecretAsync(                vaultBaseUrl: &quot;https://my-application-kv.vault.azure.net/&quot;,                secretName: &quot;MySecret&quot;);                Console.WriteLine($&quot;Ex4 AppAuthentication: {secret.Value}&quot;);    }}</code></pre><p>To configure how <code>AzureServiceTokenProvider</code> will acquire tokens you can provide a connection string, either by passing it as a parameter in the constructor or by setting it as the environment variable <code>AzureServicesAuthConnectionString</code>. If no connection string is provided, such as in my example above, the <code>AzureServiceTokenProvider</code> will try three connection strings for you and pick one that works.</p><p>If you are using the NuGet package <code>Microsoft.Extensions.Configuration.AzureKeyVault</code> to get secrets from AKV, which I wrote about in the previous example, you need to have a service principal for local development, preferably with a certificate for authentication. Yes, you can use <code>Microsoft.Extensions.Configuration.EnvironmentVariables</code> or <code>Microsoft.Extensions.Configuration.UserSecrets</code> to add secrets. But as I mentioned before, distributing them in your team might be an unnecessary pain.</p><p><code>AzureServiceTokenProvider</code> solves this rather nice. You and your team members can use your own accounts for accessing AKV, and your production application can use its own account for accessing AKV. Everything is either picked out for you, or it can be configured at deploy-time.</p><p>I have tried to summarize the supported connection string types below for you. For full details see the <a href="https://docs.microsoft.com/en-us/azure/key-vault/service-to-service-authentication#connection-string-support" target="_blank" rel="noopener">official documentation</a>.</p><h4 id="Local-development"><a href="#Local-development" class="headerlink" title="Local development"></a>Local development</h4><p>For local development scenarios, credentials can be taken from a live Azure CLI session, a logged in user in Visual Studio, or the local user account on a computer that is joined to the domain of the AKV.</p><ul><li>RunAs=Developer; DeveloperTool=AzureCli</li><li>RunAs=Developer; DeveloperTool=VisualStudio</li><li>RunAs=CurrentUser;</li></ul> Since Visual Studio 2017 Update 6 you can set the account under Tools -> Azure Service Authentication.<p>If you are not using Visual Studio, <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli" target="_blank" rel="noopener">Azure CLI 2.0</a> is the fallback. Run <code>az login</code> and go!</p><h4 id="Service-Principals"><a href="#Service-Principals" class="headerlink" title="Service Principals"></a>Service Principals</h4><ul><li>RunAs=App;AppId={AppId};TenantId=NotUsed;CertificateThumbprint={Thumbprint};CertificateStoreLocation={LocalMachine or CurrentUser}</li><li>RunAs=App;AppId={AppId};TenantId=NotUsed;CertificateSubjectName={Subject};CertificateStoreLocation={LocalMachine or CurrentUser}</li><li>RunAs=App;AppId={AppId};TenantId=NotUsed;AppKey={ClientSecret}</li></ul> When used together with `KeyVaultClient`, the TenantId part of the connection string is not used, although `AzureServiceTokenProvider` throws an exception if it not provided. This is something that will [change in an upcoming version](https://github.com/Azure/azure-sdk-for-net/issues/4169) of `Microsoft.Azure.Services.AppAuthentication`. <h4 id="Azure-Managed-Service-Identity"><a href="#Azure-Managed-Service-Identity" class="headerlink" title="Azure Managed Service Identity"></a>Azure Managed Service Identity</h4><p>Azure has a service called Managed Service Identity (MSI) which essentially provides service principals which are maintained by Azure. MSI is supported in App Service, Functions and Virtual Machines. See <a href="https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/overview" target="_blank" rel="noopener">the official docs</a> for more details.</p><ul><li>RunAs=App;</li></ul> This is fantastic! No more hassling about generating certificates or rotating secrets.<h4 id="Automatic-Mode"><a href="#Automatic-Mode" class="headerlink" title="Automatic Mode"></a>Automatic Mode</h4><p>If no connection string is provided, <code>AzureServiceTokenProvider</code> will try to resolve tokens in the following order:</p><ol><li>MSI</li><li>Visual Studio</li><li>Azure CLI</li></ol> If you use MSI in production and Visual Studio or Azure CLI for development, there is no need for any configuration. Yeah!]]></content>
    
    <summary type="html">
    
      In this post, I show four ways of how to handle secrets with .NET Core in a way so that they are stored outside of your source code repository.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Creating ARM Service Endpoints From Within VSTS</title>
    <link href="https://blog.classon.eu/2017-02-10-Creating-ARM-Service-Endpoints-from-Within-VSTS/"/>
    <id>https://blog.classon.eu/2017-02-10-Creating-ARM-Service-Endpoints-from-Within-VSTS/</id>
    <published>2017-02-09T23:00:00.000Z</published>
    <updated>2020-04-17T08:07:58.057Z</updated>
    
    <content type="html"><![CDATA[<p>First, here is some background about what Visual Studio Team Services (VSTS) can do concerning Azure Resource Manager (ARM). </p><p>As you probably know, Releases in VSTS has the capability to deploy build artifacts to subscriptions in Azure. Releases are configured through a list of tasks that performs the actual work, such as for example creating infrastructure with ARM-templates, or web-deploying applications to App Services.</p><p>VSTS lets you store connection details to Azure Subscriptions in Service Endpoints for later reuse in release tasks.</p><p>There are many Service Endpoint types, but the one of interest regarding ARM is the Azure Resource Manager Service Endpoint. When you create a new ARM Endpoint you enter its name, select one your Azure subscriptions in a dropdown, and if you are lucky, VSTS will create the connection. I will explain the prerequisites to be ?lucky? below.</p><p><img src="/2017-02-10-Creating-ARM-Service-Endpoints-from-Within-VSTS/vsts-add-arm-endpoint.png" alt="Add ARM endpoint dialog in VSTS"></p><p>The connection is actually an Azure application which use its service principal in the Azure Active Directory (Azure AD) to access subscriptions.</p><p>If you cannot find the subscription you intend to deploy to in the drop down, or if VSTS fails to create the endpoint, you can attempt to create it manually, either by creating the application and service principal by hand in the Azure Portal, or through the PowerShell API. Here is a <a href="https://blogs.msdn.microsoft.com/visualstudioalm/2015/10/04/automating-azure-resource-group-deployment-using-a-service-principal-in-visual-studio-online-buildrelease-management" target="_blank" rel="noopener">blog post by Roopesh Nair</a> that explains the procedure in more detail. It links to a <a href="https://github.com/Microsoft/vsts-rm-documentation/blob/master/Azure/SPNCreation.ps1" target="_blank" rel="noopener">PowerShell script</a> that I have used on many occasions as a base when I had to set things up manually.</p><h2 id="Manual-or-Automatic-Creation"><a href="#Manual-or-Automatic-Creation" class="headerlink" title="Manual or Automatic Creation"></a>Manual or Automatic Creation</h2><p>As you might already know, ARM service endpoints can only be shared within the team project where they are created. Since it takes a few minutes to get through the manual steps, I think that creating ARM Endpoints manually is a real pain, especially if your company use many team projects. I find it more beneficial to spend time to configure the subscriptions so that each user can let VSTS create the endpoints automatically for them, right when they are needed.</p><h2 id="Automatic-ARM-Endpoint-Creation-Prerequisites"><a href="#Automatic-ARM-Endpoint-Creation-Prerequisites" class="headerlink" title="Automatic ARM Endpoint Creation Prerequisites"></a>Automatic ARM Endpoint Creation Prerequisites</h2><p>Here is a list of prerequisites that needs to be met to make VSTS able to create applications and grant subscription rights to their service principals. Some are more obvious than others.</p><ul><li>You must have an activated Azure Subscription (Duh!). </li><li>Each subscription belongs to an Azure AD, and the account that you use to log into VSTS with needs to be present there.</li><li>The account needs to be able to create an application in the Azure AD.</li><li>The account needs to have the <code>*/read</code> and <code>Microsoft.Authorization/*/Write</code> permissions in the subscription.</li></ul><h3 id="Creating-an-Azure-Subscription"><a href="#Creating-an-Azure-Subscription" class="headerlink" title="Creating an Azure Subscription"></a>Creating an Azure Subscription</h3><p>If you have a Visual Studio Subscription or a Windows Developer account, you can activate your subscription with free credits through <a href="https://my.visualstudio.com" target="_blank" rel="noopener">https://my.visualstudio.com</a>. If you intend to use the subscription for production, create a Pay-As-You-Go subscription, preferably as a free account to start with from <a href="https://azure.microsoft.com/en-us/free" target="_blank" rel="noopener">https://azure.microsoft.com/en-us/free</a>.</p><h3 id="Adding-an-account-from-another-Azure-AD"><a href="#Adding-an-account-from-another-Azure-AD" class="headerlink" title="Adding an account from another Azure AD"></a>Adding an account from another Azure AD</h3><p>Adding users to an Azure AD has become much easier in the new portal compared to how it was done in the old. Now you can add or invite users in the same dialog.</p><p>If you enter a user email that corresponds to the Azure AD domain, a new user will be created. If you enter an email belonging to another domain, an invitation will be sent out, and the account will be added as a guest. I find it convenient to assign users that I add to the correct groups right in the add dialog.</p><h3 id="Allowing-Creation-of-Applications-in-Azure-AD"><a href="#Allowing-Creation-of-Applications-in-Azure-AD" class="headerlink" title="Allowing Creation of Applications in Azure AD"></a>Allowing Creation of Applications in Azure AD</h3><p>There is a setting named <em>Users can register applications</em> which is found under <em>Azure Active Directory - User settings</em>. If set to Yes, non-administrator users are allowed to add applications to the Azure AD. If it is set to No, then your user accounts which is going to be able to add VSTS ARM Endpoints will have to be assigned to the Global administrator directory role. To my knowledge, it is not possible to add applications with any of the Limited administrator directory roles.</p><p>If your Azure AD administrator is not fond of letting people creating applications as they like, my best advice is that you create a new directory for subscriptions, and that you add the user accounts that might create ARM endpoints in that as guests.</p><p><img src="/2017-02-10-Creating-ARM-Service-Endpoints-from-Within-VSTS/vsts-azure-ad-permissions.png" alt="User permissions in Azure Active Directory"></p><p>Guests users gets special treatment in Azure AD, and have a setting named <em>Guest users permissions are limited</em> which needs to be set to No to make them able to add applications. If it is set to Yes, it will not matter if you even assign the Global administrator role to the guest account. If you need to use guest accounts, this setting needs to be set to No. Period.</p><h3 id="Assigning-the-User-Access-Administrator-Role"><a href="#Assigning-the-User-Access-Administrator-Role" class="headerlink" title="Assigning the User Access Administrator Role"></a>Assigning the User Access Administrator Role</h3><p>The application service principals VSTS creates to use to connect to subscriptions gets Contributor rights by default. Because of this, the user that creates the ARM endpoint needs to be allowed to assign permissions in the subscription.</p><p>There are two default subscription roles that have the required permissions to do this, one is the <a href="https://docs.microsoft.com/en-us/azure/active-directory/role-based-access-built-in-roles#owner" target="_blank" rel="noopener">Owner</a> role, and the other the <a href="https://docs.microsoft.com/en-us/azure/active-directory/role-based-access-built-in-roles#user-access-administrator" target="_blank" rel="noopener">User Access Administrator</a> role.</p><p>One way to archive this could be to assign the accounts as subscription co-administrators, which would grant them the Owner role. This would work, but gets tedious if you have many accounts that should be able to create ARM endpoints. What is easier is to grant the role you intend to use using a group, let?s say one named VSTS Endpoint Managers.</p><p><img src="/2017-02-10-Creating-ARM-Service-Endpoints-from-Within-VSTS/vsts-azure-subscription-permissions.png" alt="User permissions in Azure Subscription"></p><p>If you are really lucky, you have a colleague that already maintains a directory group that you can reuse.</p>]]></content>
    
    <summary type="html">
    
      Read how you can set up you Azure subscriptions so that you can be creating ARM service endpoints from within VSTS instead of having to create them manually
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Implementing Custom Tasks in VSTS</title>
    <link href="https://blog.classon.eu/2016-11-14-Implementing-Custom-Tasks-in-VSTS/"/>
    <id>https://blog.classon.eu/2016-11-14-Implementing-Custom-Tasks-in-VSTS/</id>
    <published>2016-11-13T23:00:00.000Z</published>
    <updated>2020-04-17T08:07:38.129Z</updated>
    
    <content type="html"><![CDATA[<p>I think that one of the strengths of the build and release system in Visual Studio Team Services (VSTS) is the large variety of tasks that is available. If you ever want a task that does something that the default tasks can’t do, chances are high that someone else has made a task for just that purpose already, and that it’s ready to be installed from the <a href="https://marketplace.visualstudio.com/" target="_blank" rel="noopener">Visual Studio Marketplace</a>.</p><p>But sometimes you might be out of luck, and you have to make the work yourself. What now?</p><h2 id="Create-Task-Groups"><a href="#Create-Task-Groups" class="headerlink" title="Create Task Groups"></a>Create Task Groups</h2><p>The easiest way to create a reusable task is to create a Task Group from an already configured tasks in a build or release definition. You create a task group by selecting one or many tasks, right clicking, and selecting <em>Create task group</em>.</p><p>Here is an example of a task that I’ve made that is made up of a single <a href="https://marketplace.visualstudio.com/items?itemName=petergroenewegen.PeterGroenewegen-Xpirit-Vsts-Build-InlinePowershell" target="_blank" rel="noopener">Azure PowerShell task</a>.</p><p><img src="/2016-11-14-Implementing-Custom-Tasks-in-VSTS/Create-Tasks-in-VSTS-Task-Group-1.png" alt="Implementing Custom Tasks in VSTS - Task Group 1"></p><p>All configuration variables, <em>the ones named like <code>$(abc)</code></em>, that are present in the tasks of a task group are put as input parameters of the task group, each with a configurable default value and description.</p><p><img src="/2016-11-14-Implementing-Custom-Tasks-in-VSTS/Create-Tasks-in-VSTS-Task-Group-2.png" alt="Implementing Custom Tasks in VSTS - Task Group 2"></p><p>Task groups are great because they are so easy to make, but have the drawback of only being available in the team project where they are created.</p><h2 id="Implementing-Custom-Tasks-in-VSTS"><a href="#Implementing-Custom-Tasks-in-VSTS" class="headerlink" title="Implementing Custom Tasks in VSTS"></a>Implementing Custom Tasks in VSTS</h2><p>If you intend to share your tasks between several team projects or even accounts, your best option is to implement a custom task. Tasks in VSTS are made up of a command executable and a task manifest. The executable can be either a full blown command line application, or a simple PowerShell script. The task manifest is a json-file that contains some metadata such as the task ID, a declaration of what the configuration GUI should contain, and how the executable is invoked.</p><p>Strangely, I have not been able to find any official documentation of how to fill in the task manifest json-file. But there is an <a href="https://github.com/Microsoft/vsts-task-lib/pull/146" target="_blank" rel="noopener">active pull request</a> with a <a href="https://github.com/AArnott/vsts-task-lib/blob/taskJsonSchema/tasks.schema.json" target="_blank" rel="noopener">schema</a> that may be useful if you ever wonder what to write. </p><p>An easy way to get started is to copy one of the <a href="https://github.com/Microsoft/vsts-tasks" target="_blank" rel="noopener">default tasks of VSTS</a>, and modify it to your needs. Just remember to generate a new ID for your custom task!</p><p>The <a href="https://github.com/Microsoft/vsts-task-lib/blob/master/README.md" target="_blank" rel="noopener">documentation of the VSTS DevOps Task SDK</a> encourages you to write scripts for either the agent’s Node or PowerShell3 handlers.</p><p>Look at Microsoft’s reference tasks for guidance:</p><ul><li><a href="https://github.com/Microsoft/vsts-tasks/blob/master/Tasks/MSBuild" target="_blank" rel="noopener">MSBuild</a></li><li><a href="https://github.com/Microsoft/vsts-tasks/blob/master/Tasks/VSBuild" target="_blank" rel="noopener">VSBuild</a></li><li><a href="https://github.com/Microsoft/vsts-tasks/tree/master/Tasks/ShellScript" target="_blank" rel="noopener">ShellScript</a></li><li><a href="https://github.com/Microsoft/vsts-tasks/tree/master/Tasks/Xcode" target="_blank" rel="noopener">XCode</a></li></ul><p>Or, if you want an old school PowerShell task without that much “SDK-noise” you can copy one of mine: </p><ul><li><a href="https://github.com/johanclasson/vso-agent-tasks/tree/master/DbUpMigration" target="_blank" rel="noopener">DbUpMigration</a></li></ul><p>I think you will find that making your own custom tasks is quite straight forward.</p><h3 id="Install-Custom-Tasks-With-tfx-cli"><a href="#Install-Custom-Tasks-With-tfx-cli" class="headerlink" title="Install Custom Tasks With tfx-cli"></a>Install Custom Tasks With tfx-cli</h3><p>One way to install a task is to use the <a href="https://github.com/Microsoft/tfs-cli" target="_blank" rel="noopener">TFS Cross Platform Command Line utility (tfx-cli)</a> in a Node.js command prompt:</p><ul><li><code>npm install -g tfx-cli</code> - <em>This installs the tfx-cli tool.</em></li><li><code>tfx login</code> - <em>The login is reused throughout the entire session.</em><ul><li>Enter collection url &gt; <code>https://yourname.visualstudio.com/DefaultCollection</code></li><li>Enter personal access token &gt; <code>2lqewmdba7theldpuoqn7zgs46bmz5c2ppkazlwvk2z2segsgqrq</code> - <em>This is obviously a bogus token… You can add tokens to access your account at <a href="https://yourname.visualstudio.com/_details/security/tokens" target="_blank" rel="noopener">https://yourname.visualstudio.com/_details/security/tokens</a>.</em> </li></ul></li><li><code>tfx build tasks upload --task-path c:\path-to-repo\MyCustomTask</code><ul><li><em>If you change your mind and do not want a task anymore, you can remove it with</em> <code>tfx build tasks delete --task-id b8df3d76-4ee4-45a9-a659-6ead63b536b4</code>, <em>where the Guid is easiest found in the task.json of your task.</em></li></ul></li></ul><p>If you make a change to a task that you have previously uploaded, you have to bump its version before you upload it again.</p><h3 id="Create-Team-Services-Extensions"><a href="#Create-Team-Services-Extensions" class="headerlink" title="Create Team Services Extensions"></a>Create Team Services Extensions</h3><p>Another way to install a custom task is to package it inside an Team Services extension. You can read in the <a href="https://www.visualstudio.com/en-gb/docs/integrate/extensions/publish/overview" target="_blank" rel="noopener">official documentation how to get started</a>, or just follow these steps.</p><p>If you do not have a publisher id already, head over to the <a href="http://aka.ms/vsmarketplace-manage" target="_blank" rel="noopener">Visual Studio Marketplace Publishing Portal</a> and create it in one of the Azure directories which are associated with your account.</p><p>Create an extension manifest-file with the name <code>vss-extension.json</code>, and with content like the following:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"manifestVersion"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">"id"</span>: <span class="string">"UniqueExtensionId"</span>, <span class="comment">//FIXME</span></span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"Your extension name"</span>, <span class="comment">//FIXME</span></span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"1.2.3"</span>, <span class="comment">//FIXME</span></span><br><span class="line">    <span class="attr">"publisher"</span>: <span class="string">"yourpublisherid"</span>, <span class="comment">//FIXME</span></span><br><span class="line">    <span class="comment">//"galleryFlags": ["Public"],</span></span><br><span class="line">    <span class="attr">"targets"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"id"</span>: <span class="string">"Microsoft.VisualStudio.Services"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"A brief description which will be shown in the marketplace tile."</span>, <span class="comment">//FIXME</span></span><br><span class="line">    <span class="attr">"categories"</span>: [</span><br><span class="line">        <span class="string">"Build and release"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"tags"</span>: [ <span class="comment">//FIXME</span></span><br><span class="line">        <span class="string">"some"</span>,</span><br><span class="line">        <span class="string">"tags"</span>,</span><br><span class="line">        <span class="string">"for"</span>,</span><br><span class="line">        <span class="string">"discoverability"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"content"</span>: &#123;</span><br><span class="line">        <span class="attr">"details"</span>: &#123;</span><br><span class="line">            <span class="attr">"path"</span>: <span class="string">"relativePathTo/README.md"</span> <span class="comment">//FIXME</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"license"</span>: &#123;</span><br><span class="line">            <span class="attr">"path"</span>: <span class="string">"relativePathToLicenseFile"</span> <span class="comment">//FIXME</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"links"</span>: &#123;</span><br><span class="line">        <span class="attr">"support"</span>: &#123;</span><br><span class="line">            <span class="attr">"uri"</span>: <span class="string">"https://some-uri.com"</span> <span class="comment">//FIXME</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"branding"</span>: &#123;</span><br><span class="line">        <span class="attr">"color"</span>: <span class="string">"rgb(36, 43, 50)"</span>,</span><br><span class="line">        <span class="attr">"theme"</span>: <span class="string">"dark"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"icons"</span>: &#123;</span><br><span class="line">        <span class="attr">"default"</span>: <span class="string">"relativePathTo/extension-icon.png"</span> <span class="comment">//FIXME</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"files"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"path"</span>: <span class="string">"relativePathToTaskFolder"</span> <span class="comment">//FIXME</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"contributions"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"id"</span>: <span class="string">"UniqueIdOfTask"</span>, <span class="comment">//FIXME</span></span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"ms.vss-distributed-task.task"</span>,</span><br><span class="line">            <span class="attr">"targets"</span>: [</span><br><span class="line">                <span class="string">"ms.vss-distributed-task.tasks"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"properties"</span>: &#123;</span><br><span class="line">                <span class="attr">"name"</span>: <span class="string">"relativePathToTaskFolder"</span> <span class="comment">//FIXME</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then, run the command <code>tfx extension create</code> in a Node.js command prompt, and upload the generated .vsix-file in the publishing portal.</p><p><img src="/2016-11-14-Implementing-Custom-Tasks-in-VSTS/Create-Tasks-in-VSTS-Publishing-Portal.png" alt="Implementing Custom Tasks in VSTS - Publishing Portal"></p><p>Or if you prefer, you can use the command <code>tfx extension publish</code> instead, and supply your personal access token. </p><p>If the <code>&quot;galleryFlags&quot;: [&quot;Public&quot;]</code> setting is kept commented out, the extension will default to be a private extension, meaning that the extension will only be available in the collections you choose. Access to private extensions are managed through the publishing portal.</p><p>Once your extension is battle proven, be a good community member and make it public so that all can benefit from your work.</p>]]></content>
    
    <summary type="html">
    
      Create new tasks by implementing custom tasks in VSTS, and share them with the community! Or create tasks groups and use them in your team project.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Migrating a GitHub Repository to VSTS</title>
    <link href="https://blog.classon.eu/2016-09-02-Migrating-a-GitHub-Repository-to-VSTS/"/>
    <id>https://blog.classon.eu/2016-09-02-Migrating-a-GitHub-Repository-to-VSTS/</id>
    <published>2016-09-01T22:00:00.000Z</published>
    <updated>2020-04-17T08:07:21.161Z</updated>
    
    <content type="html"><![CDATA[<p>Regarding migrations, I have previous experience with migrating on premise Team Foundation Servers (TFS) to Visual Studio Team Services (VSTS). Those times I used tools such as the <a href="http://opshub.com/products/opshub-visual-studio-migration-utility/" target="_blank" rel="noopener">OpsHub Visual Studio Migration Utility</a> for copying work items and <a href="https://github.com/git-tfs/git-tfs" target="_blank" rel="noopener">Git TFS</a> for migrating source code.</p><p>Yes, you are right. The OpsHub utility can migrate source code as well. But since I’m a fan of Git I thought that I could do the TFVC to Git conversion when I was about to move anyway.</p><p>But enough about TFS-to-VSTS migrations. This post is about how I was trying to figure out how to approach migrating a GitHub repository to VSTS. This time, the problem was not about source code. VSTS has full support for Git, and pushing a Git repository to another remote is trivial. Here is a <a href="http://www.almguide.com/2016/02/10-steps-to-move-a-git-repo-from-github-to-visual-studio-team-services" target="_blank" rel="noopener">random blog post</a> (by Esteban Garcia) about the procedure.</p><h2 id="The-Problem-at-Hand"><a href="#The-Problem-at-Hand" class="headerlink" title="The Problem at Hand"></a>The Problem at Hand</h2><p>What I had to come up with was how to migrate Github Issues and their associated Milestones. The repository I was migrating had not done any Pull Requests, so I could disregard that type of entity completely.</p><p>When I googled around for solutions I found several attempts of using the <a href="https://developer.github.com/v3/" target="_blank" rel="noopener">GitHub REST API</a> to export issues into a CSV-format. In theory, I could use used the Excel TFS plugin to import these CSV-issues into VSTS… But, none of the scripts that I found actually worked the way that I wanted. Not even close.</p><p>So, that left me with the option of doing my own solution. Luckily, that turned out to be a good thing. The result went just the way I wanted it to be.</p><p><img src="/2016-09-02-Migrating-a-GitHub-Repository-to-VSTS/GitHubToVSTS.png" alt="Migrating a GitHub Repository to VSTS"></p><h2 id="Migrating-GitHub-Issues-to-VSTS"><a href="#Migrating-GitHub-Issues-to-VSTS" class="headerlink" title="Migrating GitHub Issues to VSTS"></a>Migrating GitHub Issues to VSTS</h2><p>The grand master plan was to use the GitHub REST API to get the information I needed, and to create iterations and work items through the VSTS REST API. To summarize, milestones were to be converted into iterations, and issues into work items.</p><h3 id="Extracting-Information-from-GitHub"><a href="#Extracting-Information-from-GitHub" class="headerlink" title="Extracting Information from GitHub"></a>Extracting Information from GitHub</h3><p>I started out by making a GET request to list all <a href="https://developer.github.com/v3/issues/#list-issues-for-a-repository" target="_blank" rel="noopener">issues for the GitHub repository</a>, but quickly found out that the result was paginated, and I only got the first 30 or so… I had to repeatedly make requests for the next page, whose URL were hiding behind a <a href="https://tools.ietf.org/html/rfc5988#page-6" target="_blank" rel="noopener">Link-header</a> in the response. It actually took me some time to figure this out, and it was not until I finally read the documentation that I discovered it. Note to self, <a href="https://en.wikipedia.org/wiki/RTFM" target="_blank" rel="noopener">RTFM</a>…</p><p>Then I noticed that most descriptions and comments where formatted in markdown. VSTS need to have its content in bare html, so I needed a way to convert the formatting somehow. Lucky me, the GitHub API has <a href="https://developer.github.com/v3/markdown/#render-a-markdown-document-in-raw-mode" target="_blank" rel="noopener">an endpoint just for that purpose</a>!</p><p>I think its raw <code>text/plain</code> method is really convenient. If I ever find myself in need for markdown conversion once more, I will definitely consider to use the GitHub API again.</p><h3 id="Creating-Work-Items-and-Iterations-in-VSTS"><a href="#Creating-Work-Items-and-Iterations-in-VSTS" class="headerlink" title="Creating Work Items and Iterations in VSTS"></a>Creating Work Items and Iterations in VSTS</h3><p>Now it was time to create some entities in VSTS, and I started with the iterations. The GitHub milestones have both title and description. Iterations just have title, so the description hade to be lost in the migration. Milestones have end date (due date, really) but lacks start date. My approach was that if a milestone had a due date set, I used the date that the milestone was created as start date for the iteration.</p><p>The hardest part was to find the name of the endpoint for iteration creation in the VSTS REST API. After some extensive research, I discovered that areas and iterations are called “<a href="https://www.visualstudio.com/en-us/docs/integrate/api/wit/classification-nodes#create-an-iteration" target="_blank" rel="noopener">classification nodes</a>“ in REST API-language.</p><p>As I tested out creating iterations, I was reminded that <a href="https://www.visualstudio.com/docs/reference/naming-restrictions#area-path-and-iteration-path" target="_blank" rel="noopener">some characters are not allowed in their names</a>. </p><p><em>I find that these restrictions are quite interesting. I can imagine why some characters are not allowed, but there are also naming restrictions. Like for example that an iteration is not allowed to be named COM1 or AUX. How on earth could the backend software be written, if the name of an entity would risk the entity to be mixed up with some random parallel port?</em></p><p>Creating work items was a real breeze. One just compose <a href="https://www.visualstudio.com/en-us/docs/integrate/api/wit/work-items#with-a-work-item-link" target="_blank" rel="noopener">a list of instructions of how the fields and states of the work item should be</a>. The only thing that was a bit troublesome was that if I sent instructions to create several comments on a work item, only the last was actually entered. My solution to that problem was to first create the work item without comments, and then update it once for each comment that needed to be added.</p><p>A very nice feature of the endpoints for creating and updating work items is <a href="https://www.visualstudio.com/en-us/docs/integrate/api/wit/work-items#make-an-update-bypassing-rules" target="_blank" rel="noopener">the bypassRules query parameter</a>. It made it possible for me to both create and update work items while having the original GitHub usernames show up in their history.</p><h2 id="Show-Me-Some-Code-Already"><a href="#Show-Me-Some-Code-Already" class="headerlink" title="Show Me Some Code Already!"></a>Show Me Some Code Already!</h2><p>The script is too long to be included in this blog post (Duh!), but here is <a href="https://github.com/johanclasson/MigrationTools" target="_blank" rel="noopener">a link to a GitHub repository of mine</a> where you can get it, and also read more about the details of what information gets copied.</p><h3 id="Be-Weary-of-Gotchas"><a href="#Be-Weary-of-Gotchas" class="headerlink" title="Be Weary of Gotchas"></a>Be Weary of Gotchas</h3><ul><li>You need to be a member of the VSTS “Project Collection Service Accounts” group to be allowed to use the bypassRules query parameter.</li><li>I used basic authentication to login on GitHub. If you have an account with activated two factor authentication that method will not work for you.</li><li>Only milestones that have issues relating to them are migrated. Empty milestones are not included.</li></ul><h2 id="Lessons-learned"><a href="#Lessons-learned" class="headerlink" title="Lessons learned"></a>Lessons learned</h2><ul><li>Read the documentation.</li><li>Writing custom migration logic is not as hard using REST APIs as with the (dreaded, and outdated) <a href="https://tfsintegration.codeplex.com" target="_blank" rel="noopener">TFS Integration Platform</a> toolkit.</li></ul>]]></content>
    
    <summary type="html">
    
      A description of how to use the REST APIs for migrating a GitHub repository to VSTS by copying miilestones and issues into iterations and work items.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Automatic Testing With an In-Memory Database and EF7</title>
    <link href="https://blog.classon.eu/2016-03-25-Automatic-Testing-with-an-In-memory-Database-and-EF7/"/>
    <id>https://blog.classon.eu/2016-03-25-Automatic-Testing-with-an-In-memory-Database-and-EF7/</id>
    <published>2016-03-24T23:00:00.000Z</published>
    <updated>2020-04-17T07:15:50.809Z</updated>
    
    <content type="html"><![CDATA[<p>If you have ever written automatic tests that runs against a real database, you might have noticed that the tests ran quite slow. To get around this problem, we instead target some kind of fake database which is kept in-memory.</p><p>To my knowledge, you could have done this in two ways while working with Entity Framework (EF).</p><p>One way has been to hide the DbContext behind some kind of abstraction. This makes it possible to replace the DbContext with simple list-objects when running an automatic test. For example the <a href="https://effort.codeplex.com/" target="_blank" rel="noopener">Effort project</a>, or the more simplistic <a href="http://blog.nethouse.se/2015/05/02/nethouse-unit-of-work-pattern/" target="_blank" rel="noopener">Nethouse UoW Pattern</a> use this approach.</p><p>Another way has been to use a SQLite in-memory database, but I have never seen a solution with working code first migrations in EF. At least not until now.</p><h2 id="New-Capabilities-of-Entity-Framework-7"><a href="#New-Capabilities-of-Entity-Framework-7" class="headerlink" title="New Capabilities of Entity Framework 7"></a>New Capabilities of Entity Framework 7</h2><p>With the coming arrival of EF7, code first migrations are finally supported with SQLite. Well? As long as you keep within the <a href="https://docs.efproject.net/en/latest/providers/sqlite/limitations.html" target="_blank" rel="noopener">technical limitations of SQLite</a> that is.</p><p>Another nice feature is the brand new InMemory database. Microsoft designed it to be a <a href="https://docs.efproject.net/en/latest/miscellaneous/testing.html" target="_blank" rel="noopener">general purpose in-memory test replacement</a>. Since it is not a relational database, things like migrations or referential integrity constraints will not work. </p><h2 id="Target-an-In-memory-Database"><a href="#Target-an-In-memory-Database" class="headerlink" title="Target an In-memory Database"></a>Target an In-memory Database</h2><p>It’s relatively straight forward how to configure a DbContext derivate to target either SQLite or InMemory. But the way you configure it in your production code might prevent you do that in your tests. In the case of the .NET framework version of EF, the usual way to configure the DbContext is by overriding a method inside the class.</p><p>So how can you tackle the problem of how to configure the DbContext to use a test database? For instance, you can’t simply solve this by setting up a configurable connection string.</p><p>The InMemory database is a different sort of provider and has to be activated by code. The SQLite in-memory could actually work with just an updated connection string, in theory that is. But a SQLite in-memory database resets each time the connection to it is closed, and the DbContext open and close the connection a lot.</p><p>One could of course do this by adding extra code to the DbContext derivate, where one could inject configuration code to target the test database. But as always, isn’t it a bad practice to keep test code in the production code?</p><p>Luckily, in EF7 it’s possible to configure the DbContext when adding Entity Framework to a <code>ServiceCollection</code> instance.  This configuration is rather verbose, but can without much effort be reused it in all your tests. This is my approach:</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Data.Common;</span><br><span class="line"><span class="keyword">using</span> Microsoft.Data.Entity;</span><br><span class="line"><span class="keyword">using</span> Microsoft.Data.Sqlite;</span><br><span class="line"><span class="keyword">using</span> Microsoft.Extensions.DependencyInjection;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">EF.TestUtils</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">sealed</span> <span class="keyword">class</span> <span class="title">TestContextFactory</span> : <span class="title">IDisposable</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> IDisposable _connection;</span><br><span class="line">        <span class="keyword">private</span> IDisposable _scope;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">int</span> _index;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">readonly</span> <span class="keyword">object</span> Locker = <span class="keyword">new</span> <span class="keyword">object</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> TContext CreateInMemoryDatabase&lt;TContext&gt;() <span class="keyword">where</span> TContext : DbContext</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">var</span> serviceCollection = <span class="keyword">new</span> ServiceCollection();</span><br><span class="line">            serviceCollection.AddEntityFramework().AddInMemoryDatabase()</span><br><span class="line">                .AddDbContext&lt;TContext&gt;(c =&gt; c.UseInMemoryDatabase());</span><br><span class="line">            <span class="keyword">var</span> serviceProvider = serviceCollection.BuildServiceProvider();</span><br><span class="line">            <span class="keyword">var</span> scope = serviceProvider.GetRequiredService&lt;IServiceScopeFactory&gt;().CreateScope();</span><br><span class="line">            _scope = scope;</span><br><span class="line">            <span class="keyword">return</span> scope.ServiceProvider.GetService&lt;TContext&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">public</span> TContext CreateInMemorySqlite&lt;TContext&gt;(<span class="keyword">bool</span> migrate = <span class="literal">true</span>) <span class="keyword">where</span> TContext : DbContext</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">string</span> connectionString = CreateSqliteSharedInMemoryConnectionString();</span><br><span class="line">            DbConnection connection = OpenConnectionToKeepInMemoryDbUntilDispose(connectionString);</span><br><span class="line">            <span class="keyword">var</span> dbContext = CreateSqliteDbContext&lt;TContext&gt;(connection);</span><br><span class="line">            <span class="keyword">if</span> (migrate)</span><br><span class="line">                dbContext.Database.Migrate();</span><br><span class="line">            <span class="keyword">return</span> dbContext;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">string</span> <span class="title">CreateSqliteSharedInMemoryConnectionString</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>        &#123;</span><br><span class="line">            <span class="keyword">string</span> name = GetUniqueName();</span><br><span class="line">            <span class="keyword">return</span> <span class="string">$"Data Source=<span class="subst">&#123;name&#125;</span>;Mode=Memory;Cache=Shared"</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">string</span> <span class="title">GetUniqueName</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>        &#123;</span><br><span class="line">            <span class="keyword">lock</span> (Locker)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">$"testdb<span class="subst">&#123;++_index&#125;</span>.db"</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">private</span> DbConnection <span class="title">OpenConnectionToKeepInMemoryDbUntilDispose</span>(<span class="params"><span class="keyword">string</span> connectionString</span>)</span></span><br><span class="line"><span class="function"></span>        &#123;</span><br><span class="line">            <span class="keyword">var</span> connection = <span class="keyword">new</span> SqliteConnection(connectionString);</span><br><span class="line">            connection.Open();</span><br><span class="line">            _connection = connection;</span><br><span class="line">            <span class="keyword">return</span> connection;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> TContext CreateSqliteDbContext&lt;TContext&gt;(DbConnection connection) <span class="keyword">where</span> TContext : DbContext</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">var</span> serviceCollection = <span class="keyword">new</span> ServiceCollection();</span><br><span class="line">            serviceCollection.AddEntityFramework().AddSqlite()</span><br><span class="line">                .AddDbContext&lt;TContext&gt;(c =&gt; c.UseSqlite(connection));</span><br><span class="line">            IServiceProvider serviceProvider = serviceCollection.BuildServiceProvider();</span><br><span class="line">            <span class="keyword">var</span> scope = serviceProvider.GetRequiredService&lt;IServiceScopeFactory&gt;().CreateScope();</span><br><span class="line">            _scope = scope;</span><br><span class="line">            <span class="keyword">return</span> scope.ServiceProvider.GetService&lt;TContext&gt;();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">Dispose</span>(<span class="params"></span>)</span></span><br><span class="line"><span class="function"></span>        &#123;</span><br><span class="line">            _connection?.Dispose();</span><br><span class="line">            _scope?.Dispose();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Note that my utility class references types from both the <code>EntityFramework.InMemory</code> and <code>EntityFramework.Sqlite</code> NuGet packages. If you have a problem with that you can split up the class, but I find it rather convenient to have both capabilities in a utils-package that I reuse in all my test projects. And since we are dealing with test-code here, I do not think it matters that much if a NuGet-package to much is brought in.</p><p>To make it possible to switch out the DbContext derivative instance when running an automatic test, make sure you inject it in your production code instead of instantiating it there.</p><p>Creating an instance of the DbContext with test configuration can be done similar to this.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> (<span class="keyword">var</span> factory = <span class="keyword">new</span> TestContextFactory())</span><br><span class="line"><span class="keyword">using</span> (<span class="keyword">var</span> context = factory.CreateInMemorySqlite&lt;MyContext&gt;(migrate: <span class="literal">true</span>))</span><br><span class="line">&#123;</span><br><span class="line">    RunScenario(context);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Or...</span></span><br><span class="line"><span class="keyword">using</span> (<span class="keyword">var</span> factory = <span class="keyword">new</span> TestContextFactory())</span><br><span class="line"><span class="keyword">using</span> (<span class="keyword">var</span> context = factory.CreateInMemoryDatabase&lt;MyContext&gt;())</span><br><span class="line">&#123;</span><br><span class="line">    RunScenario(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Avoid-Configuring-Two-Database-Providers-During-Test-Runs"><a href="#Avoid-Configuring-Two-Database-Providers-During-Test-Runs" class="headerlink" title="Avoid Configuring Two Database Providers During Test Runs"></a>Avoid Configuring Two Database Providers During Test Runs</h2><p>If you use the .NET framework version of EF, and override the <code>OnConfiguring</code> method to configure the DbContext, that method will be called even if you inject the configuratoin with help of the <code>TestContextFactory</code>. Therefore it’s a good practice to prevent cofiguration of multiple database providers in the following way.</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">internal</span> <span class="keyword">class</span> <span class="title">MyContext</span> : <span class="title">DbContext</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">override</span> <span class="keyword">void</span> <span class="title">OnConfiguring</span>(<span class="params">DbContextOptionsBuilder optionsBuilder</span>)</span></span><br><span class="line"><span class="function"></span>    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!optionsBuilder.IsConfigured) <span class="comment">// This prevents multiple configurations</span></span><br><span class="line">        &#123;</span><br><span class="line">            optionsBuilder.UseSqlite(<span class="string">@"Data Source=c:\temp\mydatabase.sqlite;"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Entity-Framework-Code-First-Migrations"><a href="#Entity-Framework-Code-First-Migrations" class="headerlink" title="Entity Framework Code First Migrations"></a>Entity Framework Code First Migrations</h2><p>Note that you do not need to run migrations when using an InMemory database. In fact, it does not even support migrations at all. If you want to test migrations, you have to use a real database such as SQLite.</p><p>Migrations are very specific to the type of database that they are written for. This probably makes it a bad idea to test SQL Server migrations on a SQLite database. EF migrations will most likely contain raw scripts when data is affected by the requested change. Remember that the DbContext does not work in a migration? Therefore, you better use the same database technology in your migration tests as you intend to use in your production code.</p><p>When you think about what database to choose, in my opinion, you should definitely consider to use SQLite. Not only because its free of charge, but also because you will have a better testing experience with it. Not all apps need the extra horsepower that SQL server brings.</p><p>A great thing with EF is that your workflow will be quite similar, whatever database you happen to choose. Now go and test that database logic!</p>]]></content>
    
    <summary type="html">
    
      EF7 introduces two viable options for testing database logic. SQLite in-memory together with code first migrations, and the new InMemory test database.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Database Migration With DbUp and VSTS</title>
    <link href="https://blog.classon.eu/2016-03-14-Database-Migration-with-DbUp-and-VSTS/"/>
    <id>https://blog.classon.eu/2016-03-14-Database-Migration-with-DbUp-and-VSTS/</id>
    <published>2016-03-13T23:00:00.000Z</published>
    <updated>2020-04-17T07:15:05.102Z</updated>
    
    <content type="html"><![CDATA[<p>All too often I find myself in projects where there is no efficient strategy around updating databases. Databases should be able to be migrated to the latest version without too much effort. If it’s too hard, developers will start sharing databases, or even worse, use a test environment database directly. </p><p>If you usually do migrations by comparing the schema of two databases, now is an opportunity for you to do something better. Besides schema and security, a database also consists of data, and data is troublesome. Large tables takes both time and resources to alter. A tool simply cannot generate resource efficient migrations, or for example figure out where the data in that dropped column should go instead.</p><p>Therefore you will always need a process or another tool to run transitional scripts beside the schema comparer. If you instead focus on the transitional script runner and have it logging which scripts that has been run to some persistent storage, you can use that log as a simple means to version your database. </p><p>Also, do not forget to include configuration data, or sample data for test and development, in your migration process.</p><h2 id="Run-Migrations-with-DbUp-and-VSTS-or-TFS-2015"><a href="#Run-Migrations-with-DbUp-and-VSTS-or-TFS-2015" class="headerlink" title="Run Migrations with DbUp and VSTS (or TFS 2015)"></a>Run Migrations with DbUp and VSTS (or TFS 2015)</h2><p>A favorite transitional script runner of mine have long been <a href="https://dbup.github.io/" target="_blank" rel="noopener">DbUp</a>. </p><blockquote><p>DbUp is a .NET library that helps you to deploy changes to SQL Server databases. It tracks which SQL scripts have been run already, and runs the change scripts that are needed to get your database up to date.</p><p>– <a href="http://dbup.readthedocs.org/en/latest/" target="_blank" rel="noopener">DbUp Documentation</a>.</p></blockquote><p>One way that you can get started with DbUp is by importing its NuGet-package in a .NET console project. But I prefer to invoke it through PowerShell. That way nothing needs to be compiled, and as long as you have access to your migration scripts you are good to go. The PowerShell way also makes a good match for deployment scenarios with Octopus Deploy or Release Management.</p><p>I have made a <a href="https://github.com/johanclasson/vso-agent-tasks#update-database-with-dbup" target="_blank" rel="noopener">VSTS Build and Release Task</a> for this purpose. But, if you would like to run DbUp elsewhere, the important part of the task is this <a href="https://github.com/johanclasson/vso-agent-tasks/blob/master/DbUpMigration/task/Update-DatabaseWithDbUp.ps1" target="_blank" rel="noopener">PowerShell script</a>.</p><h2 id="Run-Your-Tools-Often"><a href="#Run-Your-Tools-Often" class="headerlink" title="Run Your Tools Often"></a>Run Your Tools Often</h2><p>As with all deployment tools, you should run them often. The more you run them the higher the probability gets that you will have found all mistakes before deployment is made to the production environment. This does not only mean that the same tool should be run in your CI builds, but also that each developer should use it to set up their own personal database. Never underestimate the value of dogfooding your deployment tools!</p><h2 id="Non-transitional-Changes"><a href="#Non-transitional-Changes" class="headerlink" title="Non-transitional Changes"></a>Non-transitional Changes</h2><p>All database objects do not need to be changed transitionally like tables. For example regarding stored procedures and security objects, a more pragmatic approach is to keep a set of idempotent scripts that are run on each deploy. This is supported by the PowerShell script above with the <em>Journal</em> parameter. </p>]]></content>
    
    <summary type="html">
    
      PowerShell is a good way to invoke DbUp for running your migration scripts. DbUp makes it easy to keep both test- and production databases up to date.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Pay 151 SEK - Get 2400 SEK Back as Azure Credits (Microsoft Developer Program Benefit)</title>
    <link href="https://blog.classon.eu/2016-02-25-Pay-151-SEK-Get-2400-SEK-Back-as-Azure-Credits/"/>
    <id>https://blog.classon.eu/2016-02-25-Pay-151-SEK-Get-2400-SEK-Back-as-Azure-Credits/</id>
    <published>2016-02-24T23:00:00.000Z</published>
    <updated>2020-04-17T08:01:00.032Z</updated>
    
    <content type="html"><![CDATA[<p>I just recently got an email from Microsoft where they advertised one of their latest Azure drives. They now give monthly credits to all who got a Windows- and/or Windows Phone developer account.</p><p>If you have not got one already, my guess is that you will receive this benefit as well when you register. Just head to <a href="https://dev.windows.com/" target="_blank" rel="noopener">Windows Dev Center</a> and sign up. To get an account you have to pay a minor fee, equivalent to 151 SEK.</p><h2 id="Develper-Program-Benefit"><a href="#Develper-Program-Benefit" class="headerlink" title="Develper Program Benefit"></a>Develper Program Benefit</h2><p>To collect your monthly credits enter <a href="https://www.visualstudio.com/en-us/products/visual-studio-dev-essentials-vs.aspx" target="_blank" rel="noopener">Visual Studio Dev Essentials</a>, and follow the link <em>Access your benefits</em>. Then look for the Developer Program Benefit tile.</p><p><img src="/2016-02-25-Pay-151-SEK-Get-2400-SEK-Back-as-Azure-Credits/Tile.png" alt="Microsoft Developer Program Benefit tile"></p><p>(Don’t forget the 6 month free Pluralsight subscription either!)</p><p>Activating the Developer Program Benefit creates a new Azure subscription for you.</p><p><img src="/2016-02-25-Pay-151-SEK-Get-2400-SEK-Back-as-Azure-Credits/Result.png" alt="Monthly Microsoft Developer Program Benefit"></p><p>Personally, I will use this for making backups of my NAS to Azure Blob Storage. What will you do for your “free” credits? You can get a lot of fun for 200 SEK worth every month. Pay 151 SEK, and get 2400 back is IMHO a really good deal!</p>]]></content>
    
    <summary type="html">
    
      You can get a lot of fun for 200 SEK worth every month. Pay 151 SEK, and get 2400 back is IMHO a really good??Develper Program Benefit!
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>How to Use Moq With .NET Core Apps</title>
    <link href="https://blog.classon.eu/2016-02-22-How-to-use-Moq-with-.NET-Core-Apps/"/>
    <id>https://blog.classon.eu/2016-02-22-How-to-use-Moq-with-.NET-Core-Apps/</id>
    <published>2016-02-21T23:00:00.000Z</published>
    <updated>2020-04-17T07:00:29.942Z</updated>
    
    <content type="html"><![CDATA[<p>Writing <a href="http://research.microsoft.com/apps/mobile/news.aspx?post=%2Fen-us%2Fnews%2Ffeatures%2Fnagappan-100609.aspx" target="_blank" rel="noopener">automatic tests is crucial if you care about producing code with good quality</a>. And why shouldn’t you! Yes, writing tests takes time, but the effort will soon be repaid. At least if the code is going to be around for a while. Have you ever been in a project where no one dares to refactor code since it is without coverage? Come on, be a <a href="http://blog.cleancoder.com/uncle-bob/2015/11/18/TheProgrammersOath.html" target="_blank" rel="noopener">Software Professional</a>!</p><p>Enough said about that…</p><p>When I test my code I make use of a suite of tools to make the work as easy as possible. There are many tools out there, and at least I am not interested in learning to use more of them than I have to. All devs have their favorite tools for testing, and one of mine is <a href="https://github.com/moq/moq4" target="_blank" rel="noopener">Moq</a>. When a new framework or platform is released I would very much still be able to instrument the mocking framework the way I am used to. </p><p>If you try to use Moq through the official NuGet feed in an ASP.NET Core- or Universal Windows Platform (UWP) project you will find yourself in trouble. Some of the error messages you might see are:</p><ul><li><code>The dependency Moq 4.2.1510.2205 in project MyWebApplication does not support framework DNXCore,Version=v5.0</code></li><li><code>Moq 4.2.1510.2205 is not compatible with UAP,Version=v10.0</code>.</li></ul><p>Moq does simply not support .NET Core yet. Now, what to do?</p><h2 id="The-ASP-NET-Devs-at-Microsoft-to-the-Rescue"><a href="#The-ASP-NET-Devs-at-Microsoft-to-the-Rescue" class="headerlink" title="The ASP .NET Devs at Microsoft to the Rescue"></a>The ASP .NET Devs at Microsoft to the Rescue</h2><p>I have <a href="http://stackoverflow.com/questions/27918305/mocking-framework-for-asp-net-core-5-0/34933925#34933925" target="_blank" rel="noopener">come across</a> a <a href="https://github.com/aspnet/moq4" target="_blank" rel="noopener">fork of the Moq project</a> which luckily has added support for .NET Core. What is even better, there is a public NuGet feed at <code>https://www.myget.org/F/aspnetcidev/api/v3/index.json</code> where packages from that project are available. </p><p>One way to use a nonstandard NuGet package source is to create a NuGet.Config-file in the root of your solution folder looking something like this:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packageSources</span>&gt;</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="tag">&lt;<span class="name">add</span> <span class="attr">key</span>=<span class="string">"AspNetVNext"</span> <span class="attr">value</span>=<span class="string">"https://www.myget.org/F/aspnetcidev/api/v3/index.json"</span> /&gt;</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="tag">&lt;/<span class="name">packageSources</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Or you can add the AspNetVNext source-tag in your user profile NuGet file at <code>%AppData%\NuGet\NuGet.Config</code>.</p><p>Note that the Moq package with .NET Core support is in preview. To reference it, add <code>&quot;moq.netcore&quot;: &quot;4.4.0-beta8&quot;</code> as a dependency in your <code>project.json</code>-file.</p><p><em>Thank you Microsoft Devs for allowing me to use Moq on the .NET Core framework!</em></p><h2 id="xUnit-and-the-Universal-Windows-Platform"><a href="#xUnit-and-the-Universal-Windows-Platform" class="headerlink" title="xUnit and the Universal Windows Platform"></a>xUnit and the Universal Windows Platform</h2><p>Another tool which I’m fond of is xUnit. To get xUnit to work with UWP you have to add it to a Unit Test App project. Adding it to a class library the way you might be used to does not work. The procedure is described in detail in the <a href="https://xunit.github.io/docs/getting-started-uwp.html" target="_blank" rel="noopener">xUnit documentation</a>.</p>]]></content>
    
    <summary type="html">
    
      Moq does not officially support ASP.NET Core or Universal Windows platform apps yet. But you can instead use a fork that the ASP.NET Core team have made.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Automatically Stop Azure Resource Manager VMs</title>
    <link href="https://blog.classon.eu/2015-12-14-Automatically-Stop-Azure-Resource-Manager-VMs/"/>
    <id>https://blog.classon.eu/2015-12-14-Automatically-Stop-Azure-Resource-Manager-VMs/</id>
    <published>2015-12-13T23:00:00.000Z</published>
    <updated>2020-04-17T08:06:56.650Z</updated>
    
    <content type="html"><![CDATA[<p>A question that I get fairly often is how to automatically stop virtual machines in Azure so that they get deallocated and practically does not cost anything. There are many ways to do this, and in this blog post, I am going to describe how to do it with Azure Automation Runbooks.</p><p>The virtual machines that are best suited to automate starting and stopping of, are the ones that does not need to give a response quickly. Test environments, machines with background worker roles, and build infrastructure are some good examples. Production web servers are not, at least not by automation runbooks. Im sure you get that it is wiser to rely on the autoscaling load balancing for those servers instead.</p><h2 id="Different-Approaches-to-Starting-Runbooks"><a href="#Different-Approaches-to-Starting-Runbooks" class="headerlink" title="Different Approaches to Starting Runbooks"></a>Different Approaches to Starting Runbooks</h2><p>Azure has a feature called Automation that can contain runbooks. A runbook is, simply put, a PowerShell script with some extra hosting features such as logging and a platform to run on. Runbooks can be started by schedule, or through a HTTP Post webhook, or if you want to complicate things, by a PowerShell command in another runbook.</p><p>When I first started automating starting and stopping Azure resources I used daily repeating schedules because of their simplicity. But that approach has its limitations. The biggest one is that it is hard to predict when a machine is actually needed. For example, if you schedule your virtual machines to be running monday to friday from say 08:00 to 18:00 you will find that the schedule needs to be tinkered with more often that you think. Vacations, illness, and conferences are are just some occasions where you might have to disable your schedules to save some extra credits.</p><p>The better way would be to start your virtual machines first when they are really needed. And then when they are no longer used, to stop them as soon as you know this. This type of automation can be archived by starting the runbooks primarily by webhooks, and only use repeated schedules as a safety if your webhook automation would happen to fail.</p><p>The possibilities to invoke rest methods are endless. Make your phone call a start VMs webook when it connects to your work WIFI. Make traffic on a web server call a webhook to postpone your stop VMs runbook schedule. You can surely think of other interesting ways.</p><h3 id="Delayed-Webhook-Runbook-Startup"><a href="#Delayed-Webhook-Runbook-Startup" class="headerlink" title="Delayed Webhook Runbook Startup"></a>Delayed Webhook Runbook Startup</h3><p>Schedules does not need to be repeated. In fact, a schedule which is configured to start a runbook only once is a great tool to delay the startup of that runbook after a webhook has been used.</p><h2 id="How-to-Get-Started"><a href="#How-to-Get-Started" class="headerlink" title="How to Get Started"></a>How to Get Started</h2><p>In the Azure portal, you find the Automation account under <em>New - Management</em>.</p><p><img src="/2015-12-14-Automatically-Stop-Azure-Resource-Manager-VMs/Create-automation-account.png" alt="Create Automation Account"></p><p>To make it possible to reuse PowerShell scripts between automation accounts, It is convenient to link your automation account to a GitHub repository where you can store all your runbook scripts. You can do this by clicking the <em>Set Up Source Control</em> tile, entering your repository details, and then clicking on Sync. This will add a runbook for every PowerShell script, each with the authoring status <em>New</em>. To be able to run one of the imported runbooks, you have to publish it. You can do that by clicking it, then <em>Edit - Publish</em>.</p><p><img src="/2015-12-14-Automatically-Stop-Azure-Resource-Manager-VMs/Automation-account.png" alt="Automation Account"></p><p>Each automation account has a number of modules by default, which can be found under <em>Assets - Modules</em>. If your PowerShell scripts depend on another module, you can upload it there.</p><p>To make it possible to start a runbook, you click it and then click either <em>Schedule</em> or <em>Webhook</em>. You can then configure any parameters that your PowerShell script needs in that schedule or webhook instance.</p><h2 id="A-Practical-Example"><a href="#A-Practical-Example" class="headerlink" title="A Practical Example"></a>A Practical Example</h2><p>In my Visual Studio Team Services (VSTS) account, I have a project called Akkomation which I use to build a home automation software that I store the <a href="https://github.com/johanclasson/Akkomation" target="_blank" rel="noopener">source code for on GitHub</a> (more about that in a later blog post).</p><p>Each time I check in, a build is triggered in VSTS which runs the tests, and performs analysis on a SonarQube server which I host in Azure.</p><p>Since I am the only contributor to the project code is pushed rather seldom, so it does not make sense to have the SonarQube server up and running 24/7. When I do push, I am often interested to see analysis results such as code coverage and code analysis issues.</p><h3 id="Build-Workflow"><a href="#Build-Workflow" class="headerlink" title="Build Workflow"></a>Build Workflow</h3><p>The build workflow is to first check if the SonarQube server is running, and if not, start it up with a webhook. VSTS includes an <em>Azure Resource Group Deployment</em> task which can start Azure Resource Manager virtual machines. A benefit of starting the SonarQube server with a webhook is that the build can continue while it is booting up.</p><p>Just before the SonarQube analysis begins, the build waits for the service endpoint to come up. Then when the analysis is complete, an runbook which stops the virtual machine is scheduled to start three hours later.</p><p>If another build is triggered before then, the SonarQube server is reused, and the stop runbook is rescheduled to run three hours after that the second build completes.</p><p><img src="/2015-12-14-Automatically-Stop-Azure-Resource-Manager-VMs/Build-workflow.png" alt="Akkomation Build Workflow"></p><p>This workflow use two custom tasks:</p><ul><li><em>Inline PowerShell</em>, which runs a PowerShell script that is entered in the task instead of running a ps1-file as the normal PowerShell task does. I do not like to check in files in a repo just to make a build workflow possible. At least not in a small project like this one.</li><li><em>Invoke Rest Method</em>, which calls a rest method. Surprise! What it also does is that it tries to invoke it once every ten seconds until it succeeds, up until a configurable timeout. </li></ul><p>You can download both tasks at my <a href="https://github.com/johanclasson/vso-agent-tasks" target="_blank" rel="noopener">vso-agent-tasks repository</a> on GitHub.</p><p>The PowerShell script of the <em>Start SonarQube VM if Not Running</em> task is a simple one. </p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Write-Output</span> <span class="string">"Resetting Stop SonarQube VM runbook schedule."</span></span><br><span class="line"><span class="built_in">Invoke-RestMethod</span> <span class="literal">-Method</span> Post <span class="literal">-Uri</span> <span class="string">"https://s2events.azure-automation.net/webhooks?token=**********"</span> `</span><br><span class="line">    <span class="literal">-Verbose</span> | <span class="built_in">Out-Null</span></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="built_in">Write-Output</span> <span class="string">"Trying to call SonarQube service..."</span></span><br><span class="line">    <span class="built_in">Invoke-RestMethod</span> <span class="literal">-Method</span> Get <span class="literal">-Uri</span> <span class="string">"http://johanclasson-sonarqube.westeurope.cloudapp.azure.com:9000"</span> `</span><br><span class="line">        <span class="literal">-Verbose</span> | <span class="built_in">Out-Null</span></span><br><span class="line">    <span class="built_in">Write-Output</span> <span class="string">"Found it!"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="built_in">Write-Output</span> <span class="string">"Nobody seems to be at home... (<span class="variable">$_</span>)"</span></span><br><span class="line">    <span class="built_in">Write-Output</span> <span class="string">"Starting up the virtual machine."</span></span><br><span class="line">    <span class="built_in">Invoke-RestMethod</span> <span class="literal">-Method</span> Post <span class="literal">-Uri</span> <span class="string">"https://s2events.azure-automation.net/webhooks?token=**********"</span> `</span><br><span class="line">        <span class="literal">-Verbose</span> | <span class="built_in">Out-Null</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>I hope I have inspired you to do automation of starting and stopping your VMs a little smarter. Best of luck with that!</p>]]></content>
    
    <summary type="html">
    
      Use webhooks to automatically stop Azure Resource Manager VMs with runbooks. Examine an example of how this is implemented together with a VSTS build.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Fight Technical Debt With SonarQube</title>
    <link href="https://blog.classon.eu/2015-12-10-Fight-Technical-Debt-with-SonarQube/"/>
    <id>https://blog.classon.eu/2015-12-10-Fight-Technical-Debt-with-SonarQube/</id>
    <published>2015-12-09T23:00:00.000Z</published>
    <updated>2020-04-17T08:06:36.049Z</updated>
    
    <content type="html"><![CDATA[<p>The longer a project continues, the more apparently <em>innocent smells</em> are added to the source code. Eventually, you will be in a swamp which is very hard to get out of. Technical debt is a devious thing that sneaks up on you.</p><p>How would you like it if check-ins with smelly code are rejected automatically? That might be an efficient way to battle some of the technical debt anyway.</p><p>For a while now, Team Foundation Server (TFS) provides gated check-in builds. Such builds are triggered just before code is checked in. If the build fails, the check-in is rejected.</p><p>To be honest, I have never been a huge fan of gated check-ins. Most people remember to compile and run the tests before they check-in, so in my opinion, gated check-in builds were just something that were in the way.</p><p>However, with the more intelligent analysis of SonarQube, I think it is time to reevaluate that opinion and make use of gated check-in builds again.</p><h2 id="Introduction-to-SonarQube"><a href="#Introduction-to-SonarQube" class="headerlink" title="Introduction to SonarQube"></a>Introduction to SonarQube</h2><p><a href="http://www.sonarqube.org/" target="_blank" rel="noopener">SonarQube</a> is a service that can perform different forms of analysis of your project. It can run code analysis for you, such as FxCop and their own SonarLint. If you need to, <a href="https://github.com/SonarOpenCommunity/sonar-cxx/wiki/Create-Custom-Rules" target="_blank" rel="noopener">you can write custom rules</a> (in XPath format for C#) for any special validation you need to make.</p><p>Information that is produced by a build, such as code coverage and unit test execution results, can be reported to SonarQube. Analysis of that can be made on the entire code base or just on new code. Either from last analysis, or from the last version of the software.</p><p><img src="/2015-12-10-Fight-Technical-Debt-with-SonarQube/SonarQube-Example.png" alt="SonarQube Nemo Instance"></p><p>A great feature of SonarQube is that it can perform trend analysis of these project metrics. Metrics can be used both to visualize how the project has evolved over time, or to set a quality gate that code must pass before it is submitted. For example, SonarQube can be configured so that it is not ok if code coverage is lower than a certain value. It is wise to configure that threshold to be based on metrics from new or changed code since last analysis. This makes it easier to introduce such a rule midway in a project.</p><p>Another nice feature is that it is possible to see the project code with coverage highlighting. Not everyone care to run tests with coverage when they develop, and what is worse, not all have coverage software to begin with. Code coverage <a href="https://www.visualstudio.com/en-us/products/compare-visual-studio-2015-products-vs.aspx" target="_blank" rel="noopener">included in Visual Studio Enterprise</a>, or can be added with third party add-ons such as <a href="https://www.jetbrains.com/dotcover/" target="_blank" rel="noopener">dotCover</a>, but typically cost money.</p><p><em>(There are exceptions though. For example, <a href="http://testdriven.net/download.aspx" target="_blank" rel="noopener">nCover that comes bundled with TestDriven.Net</a> is free for personal and open source development.)</em></p><p>Anyhow, it is really great to have one place where all can see and reason about the current code coverage!</p><p>Here are some links to posts from the SonarQube blog:</p><ul><li><a href="http://www.sonarqube.org/mainstream-noun-the-principal-or-dominant-course-tendency-or-trend/" target="_blank" rel="noopener">Mainstream: Noun. The principal or dominant course, tendency, or trend</a></li><li><a href="http://www.sonarqube.org/do-you-care-about-your-code-track-code-coverage-on-new-code-right-now/" target="_blank" rel="noopener">Do you care about your code? Track code coverage on new code, right now!</a></li><li><a href="http://www.sonarqube.org/quality-gates-shall-your-projects-pass/" target="_blank" rel="noopener">Quality Gates: Shall your projects pass?</a></li></ul><h2 id="How-to-Get-Started"><a href="#How-to-Get-Started" class="headerlink" title="How to Get Started"></a>How to Get Started</h2><p>SonarQube is hosted as a Java application that is run inside a windows service. If you like, you can use the following powershell script to set up SonarQube on a fresh Windows Server 2012 R2.</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install Java Runtime</span></span><br><span class="line">iex ((<span class="built_in">new-object</span> net.webclient).DownloadString(<span class="string">'https://chocolatey.org/install.ps1'</span>))</span><br><span class="line">cinst jre8 <span class="literal">-y</span></span><br><span class="line"><span class="comment"># Get latest version</span></span><br><span class="line"><span class="variable">$tmpFilename</span> = [<span class="type">System.IO.Path</span>]::GetTempFileName()</span><br><span class="line"><span class="variable">$latestVersionUrl</span> = wget <span class="literal">-Uri</span> <span class="string">"http://www.sonarqube.org/downloads/"</span> <span class="literal">-UseBasicParsing</span> | </span><br><span class="line">    select <span class="literal">-ExpandProperty</span> Links | select <span class="literal">-ExpandProperty</span> href |</span><br><span class="line">    ?&#123; <span class="variable">$_</span> <span class="operator">-match</span> <span class="string">"distribution(.*)sonarqube-(.*).zip<span class="variable">$</span>"</span> &#125; | sort <span class="literal">-Descending</span> | select <span class="literal">-First</span> <span class="number">1</span></span><br><span class="line">wget <span class="variable">$latestVersionUrl</span> <span class="literal">-UseBasicParsing</span> <span class="literal">-OutFile</span> <span class="variable">$tmpFilename</span></span><br><span class="line"><span class="built_in">Add-Type</span> <span class="literal">-AssemblyName</span> System.IO.Compression.FileSystem</span><br><span class="line">[<span class="type">System.IO.Compression.ZipFile</span>]::ExtractToDirectory(<span class="variable">$tmpFilename</span>, <span class="string">'c:\SonarQube'</span>)</span><br><span class="line">rm <span class="variable">$tmpFilename</span></span><br><span class="line"><span class="comment"># Install SonarQube service</span></span><br><span class="line"><span class="variable">$installerPath</span> = <span class="built_in">Resolve-Path</span> <span class="string">"C:\SonarQube\sonarqube-*\bin\windows-x86-64\InstallNTService.bat"</span> |</span><br><span class="line">    select <span class="literal">-ExpandProperty</span> Path</span><br><span class="line">&amp; <span class="variable">$installerPath</span></span><br><span class="line"><span class="comment"># Setup firewall rule</span></span><br><span class="line"><span class="built_in">New-NetFirewallRule</span> <span class="literal">-DisplayName</span> <span class="string">"SonarQube:9000"</span> <span class="literal">-Profile</span> Any <span class="literal">-LocalPort</span> <span class="number">9000</span> <span class="literal">-Protocol</span> tcp</span><br></pre></td></tr></table></figure><p>Then, you have to manually configure SonarQube with the following steps.</p><ul><li>Change the service logon to a local administrator account.</li><li>Consider connecting a real database to SonarQube, since migration of data is only supported if you have.<ul><li>See <a href="http://redirect.sonarsource.com/doc/sq-setup-guide-for-dotnet-users.html" target="_blank" rel="noopener">ALM Ranger Guidance</a> pp 12-15, 36-46 for how to configure SonarQube with SQL Server.</li></ul></li><li>Start service.<ul><li>Wait for Java(TM) Platform SE binary-processes to stop using 100% CPU.</li></ul></li><li>Browse to <a href="http://localhost:9000" target="_blank" rel="noopener">http://localhost:9000</a>.</li><li>Login with username <em>admin</em> and password <em>admin</em>.<ul><li>Consider to change the admin password</li></ul></li><li>Install the C# plugin under Administration - System - Update Center - Available.</li><li>Restart SonarQube service.<ul><li>Wait for Java(TM) Platform SE binary-processes to stop using 100% CPU.</li></ul></li><li>Create a new project under Administration - Projects - Management.</li></ul><h2 id="How-to-configure-VSTS-TFS"><a href="#How-to-configure-VSTS-TFS" class="headerlink" title="How to configure VSTS/TFS"></a>How to configure VSTS/TFS</h2><p>Team Foundation Server (TFS) 2015 and Visual Studio Team Services (VSTS) comes with two build tasks for retrieving and sending data to a SonarQube service. In you have not got these tasks installed, you can get them at <a href="https://github.com/Microsoft/vso-agent-tasks" target="_blank" rel="noopener">vso-agent-tasks</a>.</p><p>If you use Team Foundation Version Control (TFVC), you can use gated builds to prevent bad quality check-ins.</p><p>Gated builds are not available if you use Git. In that case, use SonarQube to analyse your pull requests instead. There are plugins for <a href="http://docs.sonarqube.org/display/PLUG/GitHub+Plugin" target="_blank" rel="noopener">GutHub</a> and <a href="https://github.com/AmadeusITGroup/sonar-stash" target="_blank" rel="noopener">BitBucket</a> that will do this for you. If you use VSTS/TFS there is no problem setting up the analysis as part of a pull request build.</p><p>If you have an old version of TFS, or have on-premise build agents, you need to install the SonarScanner manually. See the <a href="http://redirect.sonarsource.com/doc/sq-setup-guide-for-dotnet-users.html" target="_blank" rel="noopener">ALM Ranger Guidance</a> pp 18-21 for instructions.</p>]]></content>
    
    <summary type="html">
    
      SonarQube is a service that can perform different forms of analysis of your project. These analysis can be used to battle some of the technical debts.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Share Code With Your Own NuGet Feeds</title>
    <link href="https://blog.classon.eu/2015-11-27-Share-Code-With-Your-Own-NuGet-Feeds/"/>
    <id>https://blog.classon.eu/2015-11-27-Share-Code-With-Your-Own-NuGet-Feeds/</id>
    <published>2015-11-26T23:00:00.000Z</published>
    <updated>2020-04-17T08:06:20.626Z</updated>
    
    <content type="html"><![CDATA[<p>I think most of us are familiar with what NuGet is, and how it can be used to add packages from the official NuGet feed to our projects. We can all agree on that It is a real convenient way of sharing code.</p><p>What is strange is that I seldom find projects that use NuGet to share code within the project. Other more ad-hoc solutions are used instead. Have you ever opened a solution in Visual Studio to find that one of the projects that is listed in the solution is also included in an other solution? If you have, you might also have suffered from:</p><ul><li><em>Missing dependent projects</em>: If adding a project as a reference to another in one solution it is easy to forget adding the referenced project to all solutions where it is now needed.</li><li><em>Long relative paths which include the branch or repository name</em>: This the just awful! No, I do not make this up. I have seen it happen more than once… Your project will no longer be able to build or use any branch strategy.</li><li><em>NuGet packages which will not restore on build:</em> NuGet packages are restored to the packages folder. By default this folder is located where the solution file is. If a project has references to DLLs in a packages folder different than the packages folder of your current solution, you will not be able to build if you have to triggered a NuGet restore on that other solution beforehand.</li></ul><p>Another (bad) solution to the problem of sharing code is to simply copy it to all locations where one needs it. This might seem easy, but it makes it very hard to manage future updates to the shared code.</p><p>My observation is that many, if not all, developers consume NuGet feeds (at least the official one), but surprisingly few produce their own.</p><h2 id="Why-You-Should-Share-Code-With-Your-Own-NuGet-Feeds"><a href="#Why-You-Should-Share-Code-With-Your-Own-NuGet-Feeds" class="headerlink" title="Why You Should Share Code With Your Own NuGet Feeds"></a>Why You Should Share Code With Your Own NuGet Feeds</h2><p>There are many reasons for not publishing packages directly to the official NuGet feed.</p><ul><li>If you do not like your packages to be public, obviously.</li><li>It does not make sense to publish packages from your CI builds.</li><li>If you publish packages that does not work, you will get bad-will from people that get an opportunity to use them.</li><li>Once packages are published in the official feed, they cannot be deleted. To correct a mistake in a published package you have to publish a new one with a higher version instead.</li></ul><p>A better solution is to use your own NuGet feed, where you have more control over which packages exist and who that can access them. Perhaps it does not even make sense to make a package available to everyone. But if it does, it is a good idea to only push it to the official NuGet feed once the package has been verified in your own feed first.</p><h2 id="Feed-Hosting-Alternatives"><a href="#Feed-Hosting-Alternatives" class="headerlink" title="Feed Hosting Alternatives"></a>Feed Hosting Alternatives</h2><p>There are many ways you can host a NuGet feed. You should use the one that seems like the best depending on your project needs.</p><p>NuGet supports two types of security. To prevent anyone from pushing packages to your feed you can set an API key, which is a simple password that will be required to provide in the push command. To prevent anyone from consuming your feed, NuGet relies on authentication. For the most part basic authentication is used, although single-sign on with active directory is also possible.</p><p>Of course, if you really want to, you can customize NuGet to also use authentication for pushing packages. For example, this would be the case if you only want users belonging to a special group to be able to contribute to a feed.</p><h3 id="On-Premise-File-Share"><a href="#On-Premise-File-Share" class="headerlink" title="On Premise - File Share"></a>On Premise - File Share</h3><p>A fast and easy way to get started is to simply put the packages in a folder which is shared within your project. NuGet calls this a Local Feed.</p><p>Good:</p><ul><li>Everyone knows how to put files in a folder.</li></ul><p>Bad:</p><ul><li>File shares are typically not suited for public distribution.</li><li>If you use Visual Studio Team Services (VSTS), you have to configure an on premise build agent for access to the file share during builds.</li><li>File shares are cumbersome for employees to access when they are not at the office.</li><li>No download statistics.</li></ul><p>Make care that no firewall rules prevent you or the build server from accessing the file share.</p><h3 id="On-Premise-ASP-NET-Web-Application"><a href="#On-Premise-ASP-NET-Web-Application" class="headerlink" title="On Premise - ASP.NET Web Application"></a>On Premise - ASP.NET Web Application</h3><p>An alternative to using file shares for on premise hosting is to use an ASP.NET web application. If you need any special access policies or special requirements regarding handling of packages, this is the way to go.</p><p>NuGet calls this a Remote Feed, but can just as well be used internally. See <a href="https://docs.nuget.org/create/hosting-your-own-nuget-feeds" target="_blank" rel="noopener">the NuGet docs for how to create the ASP.NET Web Application</a>.</p><p>Good:</p><ul><li>If put on a computer with a public IP, the feed can be used for public distribution.</li><li>Fully customizable handling of NuGet packages.</li></ul><p>Bad:</p><ul><li>If the web application cannot be reached through internet, then it is cumbersome for employees to access it when they are not at the office.</li><li>If it is desired to secure online access, some authentication work needs to be put in to secure the feed.</li><li>No built in download statistics.</li></ul><p>To access a NuGet feed in VSTS, you need to configure a generic endpoint with the url of the application and the API key as the password. To get a build to publish packages to the feed, use the NuGet Publisher task and select the generic endpoint.</p><p>If you use authentication, you need to put in the username and password into NuGet. To my knowledge, this is not supported yet by the built in NuGet tasks of VSTS. I have made a <a href="https://github.com/johanclasson/vso-agent-tasks/" target="_blank" rel="noopener">demo of how it could be done in my VSO Agent Tasks repository on GitHub</a>. Look for NuGet Publisher With Credentials.</p><p>My guess is that Microsoft will eventually provide a special type of NuGet endpoint for this type of scenario.</p><h3 id="Azure-Free-Webbapp-ASP-NET-Web-Application"><a href="#Azure-Free-Webbapp-ASP-NET-Web-Application" class="headerlink" title="Azure Free Webbapp - ASP.NET Web Application"></a>Azure Free Webbapp - ASP.NET Web Application</h3><p>Instead of putting the web application on premise, you can put it as a web application on Azure. Free web apps comes with up to 1GB of storage.</p><p>The procedure to publish an ASP.NET web application to Azure is straightforward. Make sure you have selected Azure publishing when you create the web application project, then simply right click the project and select publish.</p><p>Good:</p><ul><li>Can be used for public distribution.</li><li>Fully customizable handling of NuGet packages.</li></ul><p>Bad:</p><ul><li>If desired, authentication work is needed for security.</li><li>No built in download statistics.</li></ul><h3 id="MyGet"><a href="#MyGet" class="headerlink" title="MyGet"></a>MyGet</h3><p><a href="https://www.myget.org/" target="_blank" rel="noopener">MyGet</a> offers an advanced service for NuGet feed hosting. Their free alternative gives 500 MB of storage for public feeds protected by one or many API keys. For a minor fee, you will get 1GB of storage and the possibility to secure your feeds with authentication.</p><p>Good:</p><ul><li>Can be used for public distribution.</li><li>Advanced features like feed aggregation, feed synchronization, web hooks, policies for automatic deletion of old packages, etc.</li></ul><p>Bad:</p><ul><li>It will cost you some dollars each month to have a private feed.</li></ul><p>MyGet feeds are accessed in VSTS with generic endpoints just like the ASP.NET web application.</p><p>MyGet supports a special way to add packages to a feed. By connecting a feed to a VSTS build, MyGet can search for any NuGet packages in the build artifacts and add the ones it find to your feed. This can be a convenient way to push packages to a private feed since you do not need to handle NuGet authentication credentials.</p><p>To add NuGet packages to the build artifacts in VSTS, use the built in Publish Build Artifacts task.</p><h3 id="VSTS-Package-Feeds"><a href="#VSTS-Package-Feeds" class="headerlink" title="VSTS Package Feeds"></a>VSTS Package Feeds</h3><p>Microsoft has just recently added a preview feature to VSTS called Package Management. It is free and offers unlimited storage. <em>(Well? I have not found any listed storage quotas in their documentation. I assume its unlimited since they have not got any storage quota for source code.)</em></p><p>Because of the authentication model around VSTS, only private feeds are available.</p><p>Good:</p><ul><li>Free private feeds</li></ul><p>Bad:</p><ul><li>No way to delete published packages. Yet anyway.</li></ul><p>To get started, find <a href="https://marketplace.visualstudio.com/items/ms.feed" target="_blank" rel="noopener">Package Management in the Visual Studio Marketplace</a> and click install.</p><p>A few days ago, Microsoft patched the NuGet Publisher task to support internal NuGet feeds. When VSTS is updated next time (my guess is that it will be in the middle of december) all accounts will get the updated version, but if you do not want to wait you can <a href="https://github.com/Microsoft/vso-agent-tasks/tree/master/Tasks/NugetPublisher" target="_blank" rel="noopener">update it yourself</a>.</p><p>This is how the task should look like:</p><p><img src="/2015-11-27-Share-Code-With-Your-Own-NuGet-Feeds/UpdatedNuGetPublisherTask.png" alt="Updated Nuget Publisher Task"></p><p>Microsoft recommends that you <a href="https://www.visualstudio.com/get-started/package/use/consume#current" target="_blank" rel="noopener">consume VSTS package feeds in Visual Studio 2015 Update 1</a>. Since that version is not released yet? you can either use the <a href="https://www.visualstudio.com/en-us/news/vs2015-update1-vs.aspx" target="_blank" rel="noopener">release candidate</a>, or use the <a href="https://www.visualstudio.com/get-started/package/use/publish#auth-from-nuget" target="_blank" rel="noopener">workflow for older clients</a> which describes how to add the credentials to your local NuGet package sources.</p>]]></content>
    
    <summary type="html">
    
      To simplify sharing of code which is not yet released, you should use your own NuGet feeds. There are many options for how this can be done with VSTS.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Apply Semantic Versioning to Assemblies</title>
    <link href="https://blog.classon.eu/2015-11-23-Apply-Semantic-Versioning-to-Assemblies/"/>
    <id>https://blog.classon.eu/2015-11-23-Apply-Semantic-Versioning-to-Assemblies/</id>
    <published>2015-11-22T23:00:00.000Z</published>
    <updated>2020-04-17T08:06:03.689Z</updated>
    
    <content type="html"><![CDATA[<p>For some time now, Community TFS Build Extensions has provided a <a href="https://tfsbuildextensions.codeplex.com/SourceControl/latest#Scripts/ApplyVersionToAssemblies.ps1" target="_blank" rel="noopener">PowerShell script for automatic versioning of assemblies</a> during a build. This simple script served me well, until I heard about <a href="http://semver.org/" target="_blank" rel="noopener">semantic versioning</a>… To make a long story short, semantic versioning is a standard way to reason about version numbering.</p><p>There are quite a few version patterns that are in use, where Major.Minor.Patch.Build is a common one. This is however not a valid format according to the semantic versioning specification, although I suspect that many relates to that pattern when they talk about versioning.</p><p>The version patterns that NuGet supports are both Major.Minor.Patch and Major.Minor.Patch-Prerelease, see the <a href="https://docs.nuget.org/create/versioning" target="_blank" rel="noopener">versioning documentation of NuGet</a> for more details.  </p><p>Up until now I have struggled with setting prerelease versions for NuGet packages automatically during a build. My problem with the standard .Net assembly version, which was the version that I used to focus on, is that it does not support the prerelease pattern. Because of this, I used to include the NuGet package version either directly in the nuspec-file or as an input parameter for the build.</p><p>A couple of days ago I once again found myself in need of publishing a series of prerelease NuGet packages, and this time I found a way to make the versioning happen automatically. I discovered that .Net supports not only one, but three version types. I also found out how easy it is to create custom tools for the new build workflow in Visual Studio Team Services (VSTS) (Formerly Visual Studio Online) and TFS 2015.</p><h2 id="Customizing-the-Build-To-Version-Assemblies"><a href="#Customizing-the-Build-To-Version-Assemblies" class="headerlink" title="Customizing the Build To Version Assemblies"></a>Customizing the Build To Version Assemblies</h2><p>VSTS and TFS 2015 bring a new build workflow with a set of predefined tools organized into build steps. Needless to say, although that these tools are great they do not cover all your customization scenarios of your build- or release flow. Invoking PowerShell scripts is often an appealing option for this kind of customization. Using PowerShell scripts is especially a good idea when you are making changes that are unique for your solution. Just write your script and invoke it!</p><p>Working with builds that directly invokes PowerShell scripts has its limitations:</p><ul><li>If you have to deal with multiple arguments it is easy to get one of them wrong.</li><li>Often when you update a PowerShell script you will affect its signature, and you might have to manually update its calling command at many places.</li><li>Each build have to include the PowerShell script-files in its source code.</li></ul><p>Fortunately, it is relatively straightforward to package your scripts in a task of your own. Once such a task is installed, it can be used by all builds in the entire collection.</p><h2 id="How-to-Create-a-New-Task"><a href="#How-to-Create-a-New-Task" class="headerlink" title="How to Create a New Task"></a>How to Create a New Task</h2><p>I have not found any official documentation on how to create new tasks, but the ones that are included in VSTS/TFS 2015 by default are available as <a href="https://github.com/Microsoft/vso-agent-tasks" target="_blank" rel="noopener">open source from Microsofts GitHub account</a>. A pragmatic approach for your first homebrewed task could be to copy an existing one and customize it to your needs. Just do not forget to generate a new ID!</p><p>The minimum task should include <code>task.json</code>, <code>icon.png/svg</code> and <em>any files</em> that need to be invoked. The backend (build agents, which can be installed on both Windows and Linux) can handle running Node-, PowerShell-, and Bash scripts as well as invoking a process directly.</p><h2 id="Installation-of-Tasks"><a href="#Installation-of-Tasks" class="headerlink" title="Installation of Tasks"></a>Installation of Tasks</h2><p>The <a href="https://github.com/Microsoft/tfs-cli" target="_blank" rel="noopener">TFS Cross Platform Command Line utility (tfx-cli)</a> is used to install tasks. It is built on Node.js, so if you have not already got Node.js you have to install it. One way to do that it is to use the Chocolatey command <code>cinst nodejs</code>.</p><p>Then, to install a task run the following commands in a Node.js command prompt:</p><ul><li><code>npm install -g tfx-cli</code> - <em>This installs the tfx-cli tool.</em></li><li><code>tfx login</code> - <em>The login is reused throughout the entire session.</em><ul><li>Enter collection url &gt; <code>https://yourname.visualstudio.com/DefaultCollection</code></li><li>Enter personal access token &gt; <code>2lqewmdba7theldpuoqn7zgs46bmz5c2ppkazlwvk2z2segsgqrq</code> - <em>This is obviously a bogus token… You can add tokens to access your account at <a href="https://yourname.visualstudio.com/_details/security/tokens" target="_blank" rel="noopener">https://yourname.visualstudio.com/_details/security/tokens</a>.</em> </li></ul></li><li><code>tfx build tasks upload c:\path-to-repo\vso-agent-tasks\ApplySemanticVersioningToAssemblies</code><ul><li><em>If you change your mind and do not want a task anymore, you can remove it with</em> <code>tfx build tasks delete b8df3d76-4ee4-45a9-a659-6ead63b536b4</code>, <em>where the Guid is easiest found in the task.json of your task.</em></li></ul></li></ul><p>If you make a change to a task that you have previously uploaded, you have to bump its version before you upload it again. The server does not allow overwriting the content of an existing version.</p><h2 id="Describing-the-Apply-Semantic-Versioning-to-Assemblies-Build-Task"><a href="#Describing-the-Apply-Semantic-Versioning-to-Assemblies-Build-Task" class="headerlink" title="Describing the Apply Semantic Versioning to Assemblies Build Task"></a>Describing the Apply Semantic Versioning to Assemblies Build Task</h2><p>The solution to my NuGet package versioning problem was to use the build number to set the versions, and to configure the prerelease name in a build task. To make meaningful semantic versions of all three version types of .Net, they are set to different values. I have published the Apply Semantic Versioning to Assemblies task in my <a href="https://github.com/johanclasson/vso-agent-tasks" target="_blank" rel="noopener">VSO-Agent-Tasks GitHub repository</a> so you can make use of it.</p><p>Added as a step in a build, this is how it looks.</p><p><img src="/2015-11-23-Apply-Semantic-Versioning-to-Assemblies/ApplySemanticVersioningToAssemblies.png" alt="Apply Semantic Versioning to Assemblies User Interface"></p><h3 id="The-Three-Assembly-Versions-Supported-by-Net"><a href="#The-Three-Assembly-Versions-Supported-by-Net" class="headerlink" title="The Three Assembly Versions Supported by .Net"></a>The Three Assembly Versions Supported by .Net</h3><p>The <code>AssemblyVersion</code> is the number that is used by a dll to point out a reference to a specific version of another dll. If this version is changed in a newer dll, those references needs to be updated to target that dll instead. If you follow semantic versioning, the first two version numbers are the ones to increase when the public API changes. Therefore it is a good idea to include just those in the assembly version.</p><p>The <code>AssemblyFileVersion</code> is not used by .Net directly, but is instead of value as a historical reference. If you would ever try to figure out from what build a dll has come from, then the assembly file version would help you answer that.</p><p>The <code>AssemblyInformationalVersion</code> is something human-readable that describes a version, for example 1.0-RC1. This can theoretically be whatever text you prefer, but in this task it is only configurable to the format 1.2.3-abc0004. Note that the build number is left padded with zeros. The reason for this is that NuGet sorts prerelease versions alphabetically. Semantic versioning supports Major.Minor.Patch-Prerelease.Build, but NuGet does not. </p><p>A great thing about the <code>AssemblyInformationalVersion</code> is that NuGet will use that for the package version if it is present. Problem solved!  </p><h3 id="How-Does-It-Work"><a href="#How-Does-It-Work" class="headerlink" title="How Does It Work?"></a>How Does It Work?</h3><p>By use of a regular expression, exactly four version numbers (for example 1.2.3.4) are extracted from the build number. All AssemblyInfo.cs files are then iterated and versions are set in the following attributes:</p><ul><li><code>AssemblyVersion</code> - <em>Is set to either 1, 1.2, 1.2.3 or 1.2.3.4 depending on what you enter under “Version Numbers in AssemblyVersion-attribute”. 1.2 is the default.</em></li><li><code>AssemblyFileVersion</code> - <em>Is set to 1.2.3.4. This is not configurable.</em> </li><li><code>AssemblyInformationalVersion</code> - <em>Is set to either 1.2.3, 1.2.3-abc or 1.2.3-abc0004 depending on what you enter under “Make Release Version”, “Prerelease Name” and “Include Build Revision Number”.</em> </li></ul><p>When one of these attributes are present in the AssemblyInfo.cs-file, their entered version-string is replaced. Attributes which are not present are instead added at the end of the file.</p><p>As you well understand, this task must be placed before the build task to make any difference.</p><h3 id="Practical-Use"><a href="#Practical-Use" class="headerlink" title="Practical Use"></a>Practical Use</h3><p>The informational version format 1.2.3-abc0004, which is compatible with NuGet, can be used to represent prerelease packages from your nightly builds. For example 2.1.3-build0421 could be the semantic version for 421st build targeting the fourth bugfix of the second API update of the 2.0 release.</p><p>When packing a project package and to have NuGet use the informational version number, just set the version tag to <code>&lt;version&gt;$version$&lt;/version&gt;</code> in the nuspec-file and you are good to go. </p><p>When you are planning to make a new release, you might find that it is a good idea to have the version numbers you intend to have on release fixed and let the build revision number update until you are done. If this is the case, use <code>$(BuildDefinitionName).2.1.3$(Rev:.r)</code> as the <em>Build number format</em> for the build. When you think that you are done, you can simply tick “Make Release Version” and build to make a release version which in this case would be 2.1.3. If you would like to build a release candidate, untick “Include Build Revision Number” and replace the “Prerelease Name” to for example RC1 which would result in 2.1.3-RC1. </p><h3 id="Advanced-Options"><a href="#Advanced-Options" class="headerlink" title="Advanced Options"></a>Advanced Options</h3><p>You can change the <em>Build Number Pattern</em> that is used to extract the version numbers from the build number. If you do, then make sure that you enter matching <em>Split Characters</em> and that there would still be exactly four versions present.</p><h4 id="Other-Tools-to-Consider"><a href="#Other-Tools-to-Consider" class="headerlink" title="Other Tools to Consider"></a>Other Tools to Consider</h4><p>If you are using Git as version control, and do not mind some additional complexity, <a href="https://github.com/GitTools/GitVersion" target="_blank" rel="noopener">GitVersion</a> is an excellent tool. It fetch the version from the branch name, branch relations, commit tags and commit messages. This method of finding the version is superior to fetching it from the build number since you do not have to regularly update the build number format as you make releases.</p><p>But it forces you to follow a specific branch- and pull request strategy (although I think it’s a good one) in your development process. Not everyone would like to do that.</p><p>An other option is to use the versioning capabilities of the built in <a href="https://github.com/Microsoft/vso-agent-tasks/tree/master/Tasks/NugetPackager" target="_blank" rel="noopener">NuGet Packager task</a>. It contains an option to ?Use Build number to version package? which if checked sets a version to the NuGet package by an argument to the nuget.exe pack command. That version is extracted from the build number in a similar fashion as my Apply Semantic Versioning to Assemblies task does it, but in this case the version is fixed to be in the format Major.Minor.Patch.Build. Using this approach does not version the assemblies, nor does it support semantic versioning.</p>]]></content>
    
    <summary type="html">
    
      Apply Semantic Versioning to Assemblies during VSTS/TFS builds. Make generated NuGet packages versioned. Automate it with tasks in the new build workflow.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Git Push Only When You Have Something to Share</title>
    <link href="https://blog.classon.eu/2015-09-28-Git-Push-Only-When-You-Have-Something-to-Share/"/>
    <id>https://blog.classon.eu/2015-09-28-Git-Push-Only-When-You-Have-Something-to-Share/</id>
    <published>2015-09-27T22:00:00.000Z</published>
    <updated>2020-04-17T07:00:29.919Z</updated>
    
    <content type="html"><![CDATA[<p>Git is a source control version system that encourages you to commit changes often. Since you have your own local repository it is not that important that everything that is committed is of premium quality. You can always amend, squach, reword, and even remove commits later before you make your changes public to your team.</p><h2 id="Why-you-should-commit-often"><a href="#Why-you-should-commit-often" class="headerlink" title="Why you should commit often"></a>Why you should commit often</h2><p>It is actually a good practice to commit as soon as you get something right. Even the smallest change that works is best put into history. To commit often brings a few advantages that you would not have anyway:</p><ul><li>You never need to worry that any changes (and even temporary such) are lost if you screw up.</li><li>If you try something out, and it shows that it does not work, you can reset the branch instead of changing back the code manually.</li><li>If you later on realize that a commit is not needed you can rebase and remove that commit from history.</li><li>If you fix something important that someone else in your team suddenly needs, then that person can cherry pick your commit instead of making the change manually.</li><li>? need I go on?</li></ul><h2 id="Git-push-only-when-you-have-something-to-share"><a href="#Git-push-only-when-you-have-something-to-share" class="headerlink" title="Git push only when you have something to share"></a>Git push only when you have something to share</h2><p>Another good practice is that every branch that is public should only contain work that is complete, done, and if you will done-done. For example, one should at any time be able to do a pull request of that work and have it merged into master.</p><p>This makes it super easy for your team members to know what branches they can use to build features of their own upon. Obviously all public branches!</p><p>When using the distributed workflow that git allows, with lots of branches that are often merged all over the place, it?s easy to pick a wrong branch by mistake. The risk of that happening is lower if there are no junk cloading the things that are actually important.</p><p>In other words, invoke the git push only when you have something to share. And that something should be the kind that is worth sharing! Please keep your public repository tidy.</p><h2 id="The-fear-of-hardware-failure"><a href="#The-fear-of-hardware-failure" class="headerlink" title="The fear of hardware failure"></a>The fear of hardware failure</h2><p>One disadvantage of having lots of changes on your local machine is that they can go up in smoke if your computer dies on you. Instead of pushing commits to your public repository you need to address this in another manner.</p><p>You could set up a remote repository and push your changes there to backup. But I would not recommend it. Having more than one remote really is a hazzle! Especially when doing it for backup reasons only.</p><p>One more direct way is to simply do regular copies of your local repositories to a remote location, say a network share, or why not OneDrive? You do not need to copy your checked out branch with binaries and what not. All that is needed is the .git folder.</p><h2 id="PowerShell-to-the-rescue"><a href="#PowerShell-to-the-rescue" class="headerlink" title="PowerShell to the rescue"></a>PowerShell to the rescue</h2><p>My approach is to schedule the following PowerShell to run once each hour. Problem solved!</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">CmdletBinding</span>()]</span><br><span class="line"><span class="keyword">Param</span>(</span><br><span class="line">    [<span class="type">Parameter</span>(<span class="type">Mandatory</span>=<span class="variable">$True</span>)]</span><br><span class="line">    [<span class="built_in">string</span>]<span class="variable">$Source</span>,</span><br><span class="line">    [<span class="type">Parameter</span>(<span class="type">Mandatory</span>=<span class="variable">$True</span>)]</span><br><span class="line">    [<span class="built_in">string</span>]<span class="variable">$Target</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gci <span class="variable">$Source</span> <span class="literal">-Recurse</span> <span class="literal">-Filter</span> <span class="string">".git"</span> <span class="literal">-Force</span> <span class="literal">-Directory</span> | %&#123;</span><br><span class="line">    <span class="variable">$dir</span> = <span class="built_in">Join-Path</span> (<span class="built_in">Join-Path</span> <span class="variable">$target</span> <span class="variable">$_</span>.Parent.Name) <span class="string">".git"</span></span><br><span class="line">    robocopy <span class="variable">$_</span>.FullName <span class="variable">$dir</span> /MIR | <span class="built_in">Out-Null</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$LASTEXITCODE</span> <span class="operator">-gt</span> <span class="number">7</span>) &#123;</span><br><span class="line">        <span class="built_in">Write-Error</span> <span class="string">"RoboCopy exited with code <span class="variable">$</span>(<span class="variable">$LASTEXITCODE</span>). Something bad happended when taking backup of <span class="variable">$</span>(<span class="variable">$_</span>.Parent.FullName)!"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">Write-Output</span> <span class="string">"Mirrored content of <span class="variable">$</span>(<span class="variable">$_</span>.FullName) to <span class="variable">$dir</span>"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For example, I invoke it by <code>.\Backup-Git.ps1 -Source C:\Dev -Target C:\OneDrive\Backup\Git\Rusty</code>. </p><p>I have many computers, and I keep a separate backup folder for each. (One of them is called Rusty?)</p><p>My dev folder contains lots of clones of other people’s repositories which I am not interested in backupping. If you make it a habit to remove their .git folders then they will not get mirrored to your backup destination.</p><p>You can surely think of something else that suites your workflow better! :)</p>]]></content>
    
    <summary type="html">
    
      It&#39;s a good practice to commit often to your local repository, but invoke the git push only when you have something to share. Keep public repositories tidy!
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Pull Requests in TFS 2015</title>
    <link href="https://blog.classon.eu/2015-06-26-Pull-Requests-in-TFS-2015/"/>
    <id>https://blog.classon.eu/2015-06-26-Pull-Requests-in-TFS-2015/</id>
    <published>2015-06-25T22:00:00.000Z</published>
    <updated>2020-04-17T08:05:44.577Z</updated>
    
    <content type="html"><![CDATA[<p>We all agree on that doing code review is a good thing. But you need a natural way to conduct code reviews in your team workflow for them to happen. One such is to review all code before it’s pushed to the main branch. In a <a href="http://oss-watch.ac.uk/resources/pullrequest" target="_blank" rel="noopener">distributed version control system such as Git you can create a pull request</a> to initiate such a review. </p><p>GitHub has long supported a nice graphical experience working with pull requests, and TFS and Visual Studio Online (VSO) have just recently caught up. With the release of TFS 2015, they are now in my opinion quite similar in functionally all of them.</p><p>What is of importance when reviewing someone else’s changes is, aside from being able to comment on them, to also to know if the build still pass if they are applied. Obviously, there is no use to start the review if the changes can’t be compiled. Nor if they break any automatic tests.</p><p>In this post I will use the same <a href="https://github.com/johanclasson/PowerShell" target="_blank" rel="noopener">PowerShell repository</a> as in my <a href="http://blog.nethouse.se/2015/07/21/build-your-github-repository-with-vso/" target="_blank" rel="noopener">previous post</a>. I will use VSO, but I could just as well have used TFS 2015. For the moment, they are identical with regards to pull request functionality.</p><h2 id="Support-for-Pull-Requests-in-TFS-2015-VSO-and-GitHub"><a href="#Support-for-Pull-Requests-in-TFS-2015-VSO-and-GitHub" class="headerlink" title="Support for Pull Requests in TFS 2015, VSO and GitHub"></a>Support for Pull Requests in TFS 2015, VSO and GitHub</h2><p>If you have your automatic builds run by a build agent hosted in VSO for a Git repository which is also hosted in VSO, it is no surprise that it works out of the box.</p><p>But what about if you have your repository in GitHub? Triggering VSO builds for pull requests in a GitHub repository is something you can do, but there is no built in functionally for reporting the build results to the pull request. <a href="https://msdn.microsoft.com/Library/vs/alm/Build/github/index" target="_blank" rel="noopener">Not yet anyway</a>.</p><p>If you have your code on GitHub, then moving the source code to VSO might be something that you are not prepared to do. If so, you can host your own build server. There exists plugins for both Jenkins and TeamCity for reporting build results to a pull request on GitHub.</p><p>When preparing this post, pushing the GitHub branches of my test repository to VSO was nothing I had any objections against. Pushing all branches to another remote host can be done with the following commands: </p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote add vso https://johanclasson.visualstudio.com/DefaultCollection/_git/PowerShell</span><br><span class="line">git push <span class="literal">-u</span> vso -<span class="literal">-all</span></span><br></pre></td></tr></table></figure><h2 id="Configuring-Automatic-Building-of-Pull-Requests"><a href="#Configuring-Automatic-Building-of-Pull-Requests" class="headerlink" title="Configuring Automatic Building of Pull Requests"></a>Configuring Automatic Building of Pull Requests</h2><p>First create a build definition for your branch, if you have not already got one. Next head over to the team project settings page. Under the <em>Version Control</em> tab, you can set up something called branch policies.</p><p>Under <em>Automatically build pull requests</em> you can select the build definition that you want to be triggered.</p><p>Regarding pull requests in TFS 2015 and VSO, you can do some things which you can’t in GitHub. Under <em>Code review requirements</em> you can enforce that all commits to a branch have to be made through pull requests.</p><p>By default, the user who has made a pull request can’t approve it. If you really want this to be possible you can change this setting here as well.</p><p><img src="/2015-06-26-Pull-Requests-in-TFS-2015/2.1-Pull-Request-Settings.png" alt="Branch policies for pull requests in TFS 2015"></p><p>With <em>Add a new path</em> you can require specific reviewers for portions of your code base. For example if you got sensitive files in a specific folder, you can be sure that pull requests that change files in that folder is reviewed by an expert in your team.</p><h2 id="Working-with-Pull-Requests"><a href="#Working-with-Pull-Requests" class="headerlink" title="Working with Pull Requests"></a>Working with Pull Requests</h2><p>If you have read my <a href="http://blog.nethouse.se/2015/07/21/build-your-github-repository-with-vso/" target="_blank" rel="noopener">previous post</a> you know that in my test repository, several of the automatic tests failed. In a villainous attempt to fix this, I created a feature1 branch, and committed and pushed a change to it.</p><h3 id="Create-Pull-Requests"><a href="#Create-Pull-Requests" class="headerlink" title="Create Pull Requests"></a>Create Pull Requests</h3><p>Under the <em>CODE - Pull Requests</em> tab, you can either click <em>New Pull Request</em> in the top left, or the blue <em>Create Pull Request</em> quick button in the middle.</p><p><img src="/2015-06-26-Pull-Requests-in-TFS-2015/2.2-Create-Pull-Request.png" alt="Create a new pull request"></p><p>This opens a page where you can edit details of the pull request which it’s going to be created. Make sure that you select the correct target branch, next to <em>relative to</em>. The description field is prefilled with the commit comments from the source branch.</p><p><img src="/2015-06-26-Pull-Requests-in-TFS-2015/2.3-Edit-Pull-Request-Details.png" alt="Edit pull request details"></p><p>As you can see, I’ve cheated by commenting out the command to run the automatic tests. For sure, that would make no tests fail!  </p><h3 id="Review-Pull-Requests"><a href="#Review-Pull-Requests" class="headerlink" title="Review Pull Requests"></a>Review Pull Requests</h3><p>Luckily some decent person is likely to review the pull request before it is merged. Under <em>CODE - Pull Requests</em> you can see active pull requests. Clicking the pull request will open it in review mode, looking like this: </p><p><img src="/2015-06-26-Pull-Requests-in-TFS-2015/2.4-Pull-Request-Overview.png" alt="Review pull request"></p><p>There you can comment on source code lines and also place general comments. Note the <em>Build succeeded</em> badge in to top right, indicating that all is well. But in this case, if you inspect the <em>details</em> it will be obvious that no tests have been run.</p><p>One could click <em>Complete pull request</em> to accept- and have it merged to the main branch. Then it is a good idea to click <em>Delete source branch</em> to remove the branch, which probably will no longer be used. </p><p>But in this case one should click <em>Abandon</em>, and give the person who made the request a lesson in code ethics…</p>]]></content>
    
    <summary type="html">
    
      In this post I describe how to work with-, and set up automatic builds for pull requests in TFS 2015.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Build Your GitHub Repository With VSO</title>
    <link href="https://blog.classon.eu/2015-06-21-Build-Your-GitHub-Repository-With-VSO/"/>
    <id>https://blog.classon.eu/2015-06-21-Build-Your-GitHub-Repository-With-VSO/</id>
    <published>2015-06-20T22:00:00.000Z</published>
    <updated>2020-04-17T08:05:15.361Z</updated>
    
    <content type="html"><![CDATA[<p>If you work with an open source project, or by other means want to share your project with unknown people, GitHub is a great place to store source code. It does not provide any support for building, versioning or deployment, so you have to set these things up somewhere else.</p><p>You can use either Team City, Jenkins or TFS 2015 to host your own build service without so much trouble. If you are not so keen on running your own machine, you can use an online service instead. One of those is Visual Studio Online (VSO), which is my favorite in this case since it’s build agent:</p><ul><li>Integrates well with GitHub.</li><li>Is free to use for <a href="http://azure.microsoft.com/en-us/pricing/details/visual-studio-online/" target="_blank" rel="noopener">60 minutes each month</a>.</li><li>Can be used to build almost anything, and comes bundled with functionality for compiling .Net, Javascript, Ant, Maven, Android, Xamarin, Xcode, among others.</li><li>Is easy to customize for those special needs that you almost always have.</li></ul><p>With the new functionality introduced for TFS 2015, VSO build has become very capable. For a description about what is available read <a href="http://www.colinsalmcorner.com/post/why-you-should-switch-to-build-vnext" target="_blank" rel="noopener">Why You Should Switch to Build VNext, by Colin Dembovsky</a>.</p><p>To demonstrate the ability for customization and doing something completely different in a build, I will use my <a href="https://github.com/johanclasson/PowerShell" target="_blank" rel="noopener">PowerShell repository on GitHub</a> as an example in this post.</p><h2 id="Connecting-VSO-to-GitHub"><a href="#Connecting-VSO-to-GitHub" class="headerlink" title="Connecting VSO to GitHub"></a>Connecting VSO to GitHub</h2><p>If you do not already have an account on visualstudio.com, create one now. Once logged in create a team project representing your GitHub repository.</p><p>In TFS a team project is a container where repositories, work items, source code, and builds are kept.</p><p>Head over to Builds and create a new build definition. Enter your connection details under the Repository tab.</p><p><img src="/2015-06-21-Build-Your-GitHub-Repository-With-VSO/Build-GH-with-VSO-1.1-GitHub-Repo.png" alt="GitHub Repo"></p><p>Authorization to GitHub is made through an access token, that can be generated under <a href="http://github.com/settings/tokens" target="_blank" rel="noopener">personal access token on GitHub</a>.</p><p><img src="/2015-06-21-Build-Your-GitHub-Repository-With-VSO/Build-GH-with-VSO-1.2-GitHub-Access-Token.png" alt="GitHub Access Token"></p><h2 id="Build-Your-GitHub-Repository-With-VSO"><a href="#Build-Your-GitHub-Repository-With-VSO" class="headerlink" title="Build Your GitHub Repository With VSO"></a>Build Your GitHub Repository With VSO</h2><p>Since I used a PowerShell repository as an example there is nothing to compile in the build, but the purpose is only to run a series of tests.</p><p>To my knowledge, the most used testing framework for PowerShell is <a href="https://github.com/pester/Pester" target="_blank" rel="noopener">Pester</a>. The hosted build agents does not have the Pester module installed. So to be able to run the tests, Pester will first have to be installed. I have written a script for this, and which is explained in more detail later in this post.</p><p>What is performed by a build is configured under the build tab, by adding one of the <a href="https://msdn.microsoft.com/en-us/Library/vs/alm/Build/overview" target="_blank" rel="noopener">available build steps</a>. Running a series of Pester tests can be done by executing a PowerShell build step.</p><p><img src="/2015-06-21-Build-Your-GitHub-Repository-With-VSO/Build-GH-with-VSO-1.3-GitHub-Build-Steps.png" alt="Build Steps to Build Your GitHub Repository With VSO"></p><p>To have the build kick off automatically when changes are pushed to the Repository, configure the build with a Continuous Integration trigger. Use Filters to select what branches will trigger a build. But you can also build your GitHub repository with VSO manually by pressing the <em>Queue build</em> button.</p><h2 id="How-to-Run-Pester-in-VSO"><a href="#How-to-Run-Pester-in-VSO" class="headerlink" title="How to Run Pester in VSO"></a>How to Run Pester in VSO</h2><p><a href="https://gist.github.com/johanclasson/2e4770044f44e8c2fb43" target="_blank" rel="noopener">https://gist.github.com/johanclasson/2e4770044f44e8c2fb43</a></p><h2 id="Build-Result-and-Lessons-Learned"><a href="#Build-Result-and-Lessons-Learned" class="headerlink" title="Build Result and Lessons Learned"></a>Build Result and Lessons Learned</h2><p><img src="/2015-06-21-Build-Your-GitHub-Repository-With-VSO/Build-GH-with-VSO-1.5-Build-Failed.png" alt="Build Failed"></p><p>As can be seen, the build failed because 10 tests failed.</p><p>This was actually the first time I executed the tests on another machine than my own. It turned out that some tests weren’t as independent as I thought. And some could not even execute on a Windows Core, which is the operating system used by the VSO hosted build agents.</p><p>Running tests automatically is a good idea, and in this case I learned the hard way that it’s best to start using an external build service from the very beginning. Even for a scripting language.</p><p>One of the modules in my PowerShell repository uses web scraping to get information from a web page, and if the layout changes to much it will no longer work. To alert me when it has broken it is convenient to schedule a build every night and set up an email alert for failed builds.</p><h3 id="Build-Badge"><a href="#Build-Badge" class="headerlink" title="Build Badge"></a>Build Badge</h3><p>VSO and TFS 2015 provides a way for you to present a badge on a website to tell whether you build passes or not.</p><p>To enable this feature you can check the <em>Badge enabled</em> checkbox under the General tab of a build definition. When you save you will get an url to a SVG image.</p><p>Go build something!</p>]]></content>
    
    <summary type="html">
    
      This post describes how to build your GitHub repository with VSO. A Powershell repo is used to demonstrate doing something completely different in a build.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hyper-v Environment Templating</title>
    <link href="https://blog.classon.eu/2015-06-01-Hyper-V-Environment-Templating/"/>
    <id>https://blog.classon.eu/2015-06-01-Hyper-V-Environment-Templating/</id>
    <published>2015-05-31T22:00:00.000Z</published>
    <updated>2020-04-17T07:00:29.902Z</updated>
    
    <content type="html"><![CDATA[<p>Have you ever felt the need to have a test environment set up on your personal computer? Or perhaps to have an automated way to set up an environment in your Continuous Integration workflow? If it’s done manually, this is a tedious and I believe often error prone activity. But if it’s automated through for example my script demonstrated below, you can have your new virtual machines starting in just seconds.</p><p>I have made a <a href="https://github.com/johanclasson/PowerShell/tree/master/HyperV" target="_blank" rel="noopener">PowerShell Module</a> that contains a function that creates virtual-networks and computers in Hyper-V according to an XML-configuration. It’s idempotent enough so that it does not try to create something that is already there.</p><p>The reason that I choose Hyper-V as hypervisor is just that it’s something that is accessible for most people without any extra cost. But I believe that it should not be hard to make the module to work with another, for example Azure, if you prefer to have your environments there instead.</p><p>When developing, I prefer to have my virtual machines locally because it’s much faster. A Windows Server 2012 R2 Core instance will run with 512MB of RAM and requires about 6GB of disk space. This makes it not much of a deal to run several virtual machines even on an laptop.</p><p>In this post, I will use Windows as example since it’s the technology that I’m most familiar with. But this technique should work just as good with any other operating systems that run on Hyper-V.</p><h2 id="Alternative-Tool"><a href="#Alternative-Tool" class="headerlink" title="Alternative Tool"></a>Alternative Tool</h2><p>Microsoft already has a similar tool called <a href="http://blogs.technet.com/b/privatecloud/archive/2013/02/08/deployment-introducing-powershell-deployment-toolkit.aspx" target="_blank" rel="noopener">PowerShell Deployment Toolkit</a>, primarily for deployment of System Center 2012. Instead of using it, I preferred to write my own because I wanted something more lightweight.</p><h2 id="Setup-Windows-Server-Templates"><a href="#Setup-Windows-Server-Templates" class="headerlink" title="Setup Windows Server Templates"></a>Setup Windows Server Templates</h2><p>I prefer to work with Windows Server Core instances for two reasons. First, since they do not contain any GUI they consume less RAM and disk-space. And second, if you ever feel a need to RDP to a test server, I think it’s a sign that you have failed to fulfill Continuous Integration. Deployment of both applications and OS-configuration are done best with a tools like PowerShell DSC, Chef or Puppet. One should not have to log on to a server for deployment reasons.</p><p>To speed up creation of virtual machines one can create templates of already preconfigured instances of Windows. Then, you do not have to do the installation of the operating system manually, nor whatever other steps you normally do when creating a new virtual machine.</p><p>To create a template one should not just make a copy of an other virtual machine, but instead anonymize it first. You have to remove unique identifiers, such as for example SID and computer name. Microsoft provides a tool for this purpose called <a href="https://technet.microsoft.com/en-us/library/hh825033.aspx" target="_blank" rel="noopener">Sysprep</a>. For details about what it removes, please see <a href="https://4sysops.com/archives/why-sysprep-is-an-obligatory-windows-deployment-tool-part-1-all-the-important-sysprep-functions/" target="_blank" rel="noopener">this blog post by Michael Pietroforte</a>.</p><p>You can run Sysprep by double clicking its executive, or by invoking it through command line in the following manner.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c:\windows\system32\sysprep\sysprep.exe &#x2F;oobe &#x2F;generalize &#x2F;shutdown</span><br></pre></td></tr></table></figure><p>After Sysprep has run on a computer, and when first booting it up you need to fill in some details such as locale, administrator account password and license key.</p><h3 id="Automated-Installation"><a href="#Automated-Installation" class="headerlink" title="Automated Installation"></a>Automated Installation</h3><p>Would it not be nice if one could create virtual machines from templates without having to enter any details? Yeah, Microsoft has thought of that. There’s this feature called answer files, which enter the settings for you. You can use answer files to <a href="https://technet.microsoft.com/en-us/library/cc749415.aspx" target="_blank" rel="noopener">automate normal installations of Windows</a>, but they also work together with Syspreped images. One way of <a href="https://technet.microsoft.com/en-us/library/cc749415%28v=ws.10%29.aspx" target="_blank" rel="noopener">how to make Windows load an answer file</a> is to place it in the folder <code>C:\Windows\Panther\</code> with the name <em>unattend.xml</em>.</p><p>Once the bootup configuration is complete, the installer process removes sensible data (such as password and product key) from that file by replacing them with <code>*SENSITIVE*DATA*DELETED*</code>.</p><p>There’s an official tool from Microsoft for creating answer files which is named Windows System Image Manager (SIM), and it’s part of the <a href="https://www.microsoft.com/en-US/download/confirmation.aspx?id=39982" target="_blank" rel="noopener">Windows Assessment and Deployment Kit</a>. Personally, I think that this application is a mess. It’s way to complicated to make me even want to try to use it.</p><p>The site <a href="http://windowsafg.no-ip.org" target="_blank" rel="noopener">Windows Answer File Generator</a> comes to rescue. From there you can generate answer files for all Windows operating systems, but with the limitation that it does not offer the same flexibility as Windows SIM. For example, it can not add instructions for <a href="http://www.windows-noob.com/forums/index.php?/topic/578-how-can-i-join-a-domain-using-windows-sim/" target="_blank" rel="noopener">automatically joining a domain</a>.</p><p>In the <a href="https://raw.githubusercontent.com/johanclasson/PowerShell/master/HyperV/unattend-ws2012r2.xml" target="_blank" rel="noopener">answer file I used preparing this post</a>, I inserted <em>underscore tokens</em> in place of some fields. The function in the PowerShell Module replace them when creating virtual machines from that template.</p><p>To create the <em>unattend.xml</em> file on your template VHDX you can mount it with PowerShell, for example in the following way.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Mount-DiskImage C:\Hyper-V\Templateswin2012r2_template.vhdx</span><br><span class="line">$url &#x3D; &quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;johanclasson&#x2F;PowerShell&#x2F;master&#x2F;HyperV&#x2F;unattend-ws2012r2.xml&quot;</span><br><span class="line">Invoke-WebRequest $url -OutFile F:\Windows\Panther\unattend.xml</span><br><span class="line">Dismount-DiskImage C:\Hyper-V\Templateswin2012r2_template.vhdx</span><br></pre></td></tr></table></figure><h3 id="Alternative-Way-to-Create-Template-VHDX"><a href="#Alternative-Way-to-Create-Template-VHDX" class="headerlink" title="Alternative Way to Create Template VHDX"></a>Alternative Way to Create Template VHDX</h3><p>When creating your template, instead of installing the operating system and runing Sysprep manually you can use a tool. I have found the TechNet Scriptcenter <a href="https://gallery.technet.microsoft.com/scriptcenter/Convert-WindowsImageps1-0fe23a8f" target="_blank" rel="noopener">Convert-WindowsImage.ps1</a> which can create sysprepped VHDX images from .iso-files.</p><p>I find this script to be too massive to use, and instead prefer to do the template myself. Creating templates is not something that I do often, so I do not have so much to gain from automating that activity.</p><h2 id="Invoking-the-Hyper-V-Environment-Templating-Function"><a href="#Invoking-the-Hyper-V-Environment-Templating-Function" class="headerlink" title="Invoking the Hyper-V Environment Templating Function"></a>Invoking the Hyper-V Environment Templating Function</h2><p>To run the PowerShell function which creates the virtual machines you can either install the module, or just copy the .ps1-file and run it as a script.</p><p>A <a href="http://blogs.technet.com/b/heyscriptingguy/archive/2010/01/21/hey-scripting-guy-january-21-2010.aspx" target="_blank" rel="noopener">PowerShell Module</a> is to some extent some functions that someone has packaged together. PowerShell monitors the folders in the environment variable <code>PSModulePath</code> for modules, and automatically imports them right when they you need them.</p><p>If you choose to install it as a module you have to rename the .ps1-files to .psm1, or you can follow the instructions in my PowerShell Repo Readme under <a href="https://github.com/johanclasson/PowerShell/blob/master/README.md#get-started" target="_blank" rel="noopener">Getting started</a>. I prefer to have my module scripts as .ps1-files when I develop them. Once I decide to install a new version of a module, I have them renamed in the install process.</p><p>To invoke the Hyper-V Environment Templating function, type <code>New-VMFromConfig -Path C:\PathTo\MyConfig.xml</code>. I you like you can use the <code>-Config</code> parameter instead and pass in an <code>[xml]</code> object.</p><p>The XML content might look something like this:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Config</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">VMSwitches</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">InternalVMSwitch</span> <span class="attr">name</span>=<span class="string">"LocalComputer"</span> <span class="attr">dns</span>=<span class="string">"10.0.0.1"</span> <span class="attr">ip</span>=<span class="string">"10.0.0.2"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ExternalVMSwitch</span> <span class="attr">name</span>=<span class="string">"ExternalEthernet"</span> <span class="attr">netAdapterName</span>=<span class="string">"Ethernet"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- I did not bother to handle private network switches since I never use them. --&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">VMSwitches</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">VMs</span> <span class="attr">root</span>=<span class="string">"C:\Hyper-V"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Example 1: Create VM from Template --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">VM</span> <span class="attr">name</span>=<span class="string">"Dev-Frontend"</span> <span class="attr">switches</span>=<span class="string">"LocalComputer,ExternalEthernet"</span></span></span><br><span class="line"><span class="tag">        <span class="attr">startupBytes</span>=<span class="string">"512MB"</span> <span class="attr">dynamicMemory</span>=<span class="string">"false"</span> <span class="attr">processorCount</span>=<span class="string">"4"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">MoveVhd</span> <span class="attr">path</span>=<span class="string">"C:\Hyper-V\Templates\Copies\win2012r2*.vhdx"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ReplaceContent</span> <span class="attr">pathRelativeRoot</span>=<span class="string">"Windows\Panther\unattend.xml"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">add</span> <span class="attr">key</span>=<span class="string">"__Locale__"</span> <span class="attr">value</span>=<span class="string">"041d:0000041d"</span> /&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">add</span> <span class="attr">key</span>=<span class="string">"__TimeZone__"</span> <span class="attr">value</span>=<span class="string">"W. Europe Standard Time"</span> /&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">add</span> <span class="attr">key</span>=<span class="string">"__ComputerName__"</span> <span class="attr">value</span>=<span class="string">"Frontend"</span> /&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">add</span> <span class="attr">key</span>=<span class="string">"__ProductKey__"</span> <span class="attr">value</span>=<span class="string">"AAAAA-BBBBB-CCCCC-DDDDD-EEEEE"</span> /&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">add</span> <span class="attr">key</span>=<span class="string">"__Password__"</span> <span class="attr">value</span>=<span class="string">"p@ssw0rd"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">ReplaceContent</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">MoveVhd</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">VM</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Example 2: Create VM with dvd and new disk --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">VM</span> <span class="attr">name</span>=<span class="string">"Dev-Backend"</span> <span class="attr">switches</span>=<span class="string">"LocalComputer"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">DvdDrive</span> <span class="attr">path</span>=<span class="string">"C:\iso\dvd.iso"</span> /&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">Vhd</span> <span class="attr">size</span>=<span class="string">"120GB"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">VM</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">VMs</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Config</span>&gt;</span></span><br></pre></td></tr></table></figure><p>New virtual machines are created under the <code>&lt;VMs&gt;</code> root attribute in a folder that matches the name of the virtual machine.</p><p>The supported VHD tags are <code>&lt;MoveVhd path=&quot;...&quot;&gt;</code>, <code>&lt;CopyVhd path=&quot;...&quot;&gt;</code>, <code>&lt;DvdDrive path=&quot;...&quot;&gt;</code>, and <code>&lt;Vhd size=&quot;...&quot;&gt;</code>. The latter will create a new disk of the specified size. All three VHD tags can have a filename attribute which if set, specifies the name of the VHD file in the virtual machine folder.</p><p>Both the <code>&lt;CopyVhd&gt;</code> and <code>&lt;MoveVhd&gt;</code> supports the <code>&lt;ReplaceContent&gt;</code> tag which makes the answer file <em>underscore token</em> update possible. An advantage of moving a template VHD over copying it, is that it’s faster. The downside is that the template VHDs will no longer be available for future use. But if you create copies of your template VHD in advance you can overcome this. Since the VHD size of a clean install of Windows Server 2012 R2 Core is so small, space should not be an issue.</p><p>The <code>&lt;MoveVhd&gt;</code> path attribute supports wildcard matching. It picks the first file that matches the expression.</p><h2 id="Tricks-and-Tweaks"><a href="#Tricks-and-Tweaks" class="headerlink" title="Tricks and Tweaks"></a>Tricks and Tweaks</h2><h3 id="Installing-Windows-Server-2012-R2-AD-DNS-and-DHCP"><a href="#Installing-Windows-Server-2012-R2-AD-DNS-and-DHCP" class="headerlink" title="Installing Windows Server 2012 R2 AD, DNS and DHCP"></a>Installing Windows Server 2012 R2 AD, DNS and DHCP</h3><p>Name resolution and IP address distribution in Hyper-V is poor without a dedicated DNS and DHCP within the network. If you run your virtual machines on an external network, all is fine. But if you use internal or private networks you have to host these services yourself. Your would normally also need to connect to an Active Directory.  </p><p>To solve these problems all in one you can configure an instance of Windows Server 2012 R2 to act as both AD, DNS and DHCP-server.</p><p>I found a <a href="https://myvirtualcloud.wordpress.com/2012/12/21/installing-windows-server-2012-ad-dns-and-dhcp/" target="_blank" rel="noopener">blog post written by Ivo Bruinewoud</a> which describe an easy way to do this with PowerShell. Here is a slightly modified version:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Static IP</span><br><span class="line">New-NetIPAddress 10.0.0.1 -InterfaceAlias &quot;Ethernet&quot; -PrefixLength 24</span><br><span class="line">Set-DnsClientServerAddress -InterfaceAlias &quot;Ethernet&quot; -ServerAddresses 127.0.0.1</span><br><span class="line"></span><br><span class="line"># AD</span><br><span class="line">Install-WindowsFeature AD-Domain-Services -IncludeManagementTools</span><br><span class="line">Install-ADDSForest -DomainName classon.eu</span><br><span class="line"></span><br><span class="line"># DHCP (granted that the name of the computer is DC01)</span><br><span class="line">Install-WindowsFeature DHCP -IncludeManagementTools</span><br><span class="line">Add-DhcpServerv4Scope -name &quot;My Scope&quot; -StartRange 10.0.0.100 -EndRange 10.0.0.200 -SubnetMask 255.255.255.0</span><br><span class="line">Set-DhcpServerv4OptionValue -DnsDomain classon.eu -DnsServer 10.0.0.1</span><br><span class="line">Add-DhcpServerInDC -DnsName dc01.classon.eu</span><br><span class="line"></span><br><span class="line"># User</span><br><span class="line">New-ADUser -SamAccountName johan -AccountPassword (ConvertTo-SecureString &quot;p@ssw0rd&quot; -AsPlainText -Force) -name &quot;johan&quot; -enabled $true -PasswordNeverExpires $true -ChangePasswordAtLogon $false</span><br><span class="line">Add-ADPrincipalGroupMembership -Identity &quot;CN&#x3D;johan,CN&#x3D;Users,DC&#x3D;classon,DC&#x3D;eu&quot; -MemberOf &quot;CN&#x3D;Enterprise Admins,CN&#x3D;Users,DC&#x3D;classon,DC&#x3D;eu&quot;,&quot;CN&#x3D;Domain Admins,CN&#x3D;Users,DC&#x3D;classon,DC&#x3D;eu&quot;</span><br></pre></td></tr></table></figure><p>The PowerShell command to join a client machine to a domain is:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Execute on the client machine</span><br><span class="line">Add-Computer -DomainName classon.eu -Restart</span><br></pre></td></tr></table></figure><h3 id="Switching-Between-GUI-and-No-GUI-in-Windows-Server"><a href="#Switching-Between-GUI-and-No-GUI-in-Windows-Server" class="headerlink" title="Switching Between GUI and No GUI in Windows Server"></a>Switching Between GUI and No GUI in Windows Server</h3><p>Sometimes it will be a pain to try to configure something that you know is easy if you have a GUI, but which isn’t when you are on a Windows Server Core instance. In those cases you can use <code>Install-WindowsFeature</code> to install the GUI, just to make your configuration, and then uninstall it again with <code>Uninstall-WindowsFeature</code> when you’re done.</p><p>If you invoke the Get-WindowsFeature on a Core instance, you will find the GUI related features looking like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Display Name                                            Name                       Install State</span><br><span class="line">------------                                            ----                       -------------</span><br><span class="line">[X] User Interfaces and Infrastructure                  User-Interfaces-Infra          Installed</span><br><span class="line">    [ ] Graphical Management Tools and Infrastructure   Server-Gui-Mgmt-Infra            Removed</span><br><span class="line">    [ ] Desktop Experience                              Desktop-Experience               Removed</span><br><span class="line">    [ ] Server Graphical Shell                          Server-Gui-Shell                 Removed</span><br></pre></td></tr></table></figure><p>And if invoked on a full installation, they look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Display Name                                            Name                       Install State</span><br><span class="line">------------                                            ----                       -------------</span><br><span class="line">[X] User Interfaces and Infrastructure                  User-Interfaces-Infra          Installed</span><br><span class="line">    [X] Graphical Management Tools and Infrastructure   Server-Gui-Mgmt-Infra          Installed</span><br><span class="line">    [ ] Desktop Experience                              Desktop-Experience             Available</span><br><span class="line">    [X] Server Graphical Shell                          Server-Gui-Shell               Installed</span><br></pre></td></tr></table></figure><p>If a Windows Feature is <em>Available</em>, or <em>Removed</em> and you have got access to the internet, then you can install it without having any installation media. If that is not the case, you have to insert the .iso-file with the same version of the operating system in the DVD drive and specify the <code>Source</code> parameter. Follow <a href="https://support.microsoft.com/en-us/kb/2913316" target="_blank" rel="noopener">these instructions</a> for guidance.</p><h3 id="Setting-up-PowerShell-Remoting"><a href="#Setting-up-PowerShell-Remoting" class="headerlink" title="Setting up PowerShell Remoting"></a>Setting up PowerShell Remoting</h3><p>When you deal with servers that has not got the features <em>Server-Gui-Shell</em> or <em>Server-Gui-Mgmt-Infra</em> activated, I think that it’s cumbersome to use remote desktop or the Hyper-V Virtual Machine Connection to connect to them. A more elegant solution is to use PowerShell Remoting, although it can be a bit hard to configure when not connected to a domain, such as in a workspace environment or when your try to connect to a machine from outside a domain. When connecting to a domain in an virtual isolated network from your host machine this will essentially be the case.</p><p>To get PowerShell Remoting to work on my Windows 8.1 laptop, I had to run the following commands:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Enable-PSRemoting ?Force -SkipNetworkProfileCheck</span><br><span class="line">Set-Item wsman:localhost\client\trustedhosts -Value *</span><br></pre></td></tr></table></figure><p>Depending on the network confirmation of your host computer, you might not need the <code>-SkipNetworkProfileCheck</code> parameter.</p><h3 id="Setting-up-Name-Resolution-to-Work-with-Virtual-Network"><a href="#Setting-up-Name-Resolution-to-Work-with-Virtual-Network" class="headerlink" title="Setting up Name Resolution to Work with Virtual Network"></a>Setting up Name Resolution to Work with Virtual Network</h3><p>To make name resolution between your host and virtual machines to work, they need to be on the same subnet. Network Discovery is by default rejected by the windows firewall. You can enable it with the PowerShell command <code>Enable-NetFirewallRule NETDIS-NB_Name-In-UDP</code>.</p><p>I like to have my machines answering to Ping. When you are already at it, enable the firewall rule <code>FPS-ICMP4-ERQ-In</code> as well.</p><p>If you set the DNS address of the network adapter to be that of the DNS server, you can use the DNS server for name resolution. But since your host machine is outside the domain, you have to use the complete domain address. For example, <code>frontend.classon.eu</code>.</p><p>Also, PowerShell Remoting will not work if the IP address of the host is on another subnet than of the virtual machines.</p><h2 id="Future-Improvement"><a href="#Future-Improvement" class="headerlink" title="Future Improvement"></a>Future Improvement</h2><p>It would be nice to be able to have virtual machines being able to automatically join a domain.</p><p>For this to be possible, I have to solve two problems. First, I have to update the answer file to also include instructions for how to join a domain. Either I have to click through the Windows SIM application, or find someone else that has managed to do it and copy some lines from their xml-file. TechNet has some <a href="https://technet.microsoft.com/sv-se/library/cc732280.aspx" target="_blank" rel="noopener">sample answer files</a> for older operating systems that seams interesting. Hopefully they will also work for Windows Server 2012 R2.</p><p>Second, I need to make a PowerShell function that can start virtual machines. When creating a new environment with network isolation there is obviously no domain controller already present. Hence there might be an error if they start simultaneously. The function can solve this by waiting for any DCs to boot up before it starts any other virtual machines.</p><p>As an alternative to using answer files for domain joining, I might consider to use PowerShell instead. What I have in mind is to wait for a DC to start up, and then invoking the domain join on the client machines through a remoting session.</p><p>I will try to address this in a blog post sometime in the future.</p>]]></content>
    
    <summary type="html">
    
      This post describe how to automate setting up virtual machines with Hyper-V Environment Templating in your CI flow, so they can be starting in just seconds.
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Gör Ditt Nästa Certifieringstest För Microsoft Från Din Egen Dator!</title>
    <link href="https://blog.classon.eu/2015-05-27-G%C3%B6r-ditt-n%C3%A4sta-certifieringstest-f%C3%B6r-Microsoft-fr%C3%A5n-din-egen-dator!/"/>
    <id>https://blog.classon.eu/2015-05-27-G%C3%B6r-ditt-n%C3%A4sta-certifieringstest-f%C3%B6r-Microsoft-fr%C3%A5n-din-egen-dator!/</id>
    <published>2015-05-26T22:00:00.000Z</published>
    <updated>2020-04-17T07:22:09.030Z</updated>
    
    <content type="html"><![CDATA[<p>Att göra certifieringstest för Microsoft via tredjepartsleverantörer är krångligt. Särskilt när man bor i en lite mindre stad, så som exempelvis Örebro. Visserligen har vi nyligen fått ett Addskills Testcenter här, men det erbjuder bara att skriva certifieringstest en gång i månaden.</p><p>När Microsoft bytte leverantör av certifieringstest från Prometric till Pearson VUE erbjuds ett nytt sätt att göra proven, nämligen via så kallade <em>Online Proctored Exams</em>. Även om det är lite krångel med att få igång det tekniska är det värt att överväga att göra sina examen på det här sättet. Och då inte bara på grund av förenklad logistik. Kostnaden för ett certifieringstest via Addskills är beroende på ort mellan 2 000 - 3 000 SEK. Gör man det via ett <em>Online Proctored Exam</em> kostar det istället 125 EUR. </p><h2 id="Hur-bokar-man-certifieringstest-for-Microsoft"><a href="#Hur-bokar-man-certifieringstest-for-Microsoft" class="headerlink" title="Hur bokar man certifieringstest för Microsoft?"></a>Hur bokar man certifieringstest för Microsoft?</h2><p>För att boka sitt provtillfälle går man in på <a href="https://www.microsoft.com/learning/en-us/" target="_blank" rel="noopener">Microsoft Learning (en-us)</a>. Notera att länken går till den amerikanska varianten. Märkligt nog så kommer man till en felsida om man använder <a href="https://www.microsoft.com/learning/" target="_blank" rel="noopener">Microsoft Learning</a>. Det har varit så här i alla fall under senaste månaden. Gissningsvis så är det en regional sida för besökare från Sverige som är trasig.</p><p>Väl där klickar du på <em>Register for exam</em>, och följer guiden för att göra certifieringstestet som ett <em>Online Proctored Exam</em>.</p><p>Notera adressuppgifterna som du anger under bokningen. Om du blir tvungen att ringa supporten kommer du behöva uppge dessa för att kunna styrka din identitet.</p><h2 id="Hur-gor-man"><a href="#Hur-gor-man" class="headerlink" title="Hur gör man?"></a>Hur gör man?</h2><p>Programvaran för certifieringstestet installerar du genom att göra <em>System Test 1</em> på <a href="http://www.pearsonvue.com/microsoft/op/" target="_blank" rel="noopener">Pearson VUEs Microsoft-sida</a>. Där finns även information om regler och teknikaliteter som rör testet.</p><p>Under själva tillfället blir man övervakad via webbkamera och mikrofon. Innan så ombeds man filma runt i rummet så att det inte finns några fusklappar eller dylikt. Av den här anledningen är det smidigt att ha en extern webbkamera så att det är lättare att filma i olika vinklar. Att bära runt en bärbar dator fungerar, men är lite klumpigt. Särskilt om man, som Pearson VUE rekommenderar, använder ethernet-sladd istället för WIFI.</p><p>Man måste också visa upp legitimation för kontrollanten. Denne behöver kunna läsa av giltighetstiden utan att bilden är suddig. Därför behöver man ha en webbkamera med relativt kort fokusavstånd. Vanligtvis är det bara webbkameror med autofokus som klarar av detta. </p><p>Jag rekommenderar <a href="https://www.microsoft.com/hardware/sv-se/p/lifecam-studio#details" target="_blank" rel="noopener">Microsoft LifeCam Studio</a>. Runt 600kr för en webbkamera kan verka överdrivet, men den är faktiskt värd varenda krona. Dessutom behöver man inte installera några drivrutiner manuellt för att använda den här kameran. Det är bara att plugga in och köra!</p><h2 id="Hur-startar-man-sjalva-testet"><a href="#Hur-startar-man-sjalva-testet" class="headerlink" title="Hur startar man själva testet?"></a>Hur startar man själva testet?</h2><p>På <a href="https://www.microsoft.com/learning/en-us/dashboard.aspx" target="_blank" rel="noopener">Microsoft Learning Dashboard</a> kan man starta ett inbokat certifieringstest via länken <em>Start a previously scheduled online proctored exam</em>.</p><p>Innan du börjar testet är det är lämpligt att kontrollera att webbkameran och mikrofonen fungerar, samt att stänga ner övriga program på din dator så som exempelvis Skype och Outlook. Pearson VUE vill även att man temporärt ska stänga av sitt antivirusprogram.</p><p>Värt att notera är att tiden som är angiven för provtillfället är i <a href="http://en.wikipedia.org/wiki/Central_European_Time" target="_blank" rel="noopener">CET</a> eller <a href="http://en.wikipedia.org/wiki/Central_European_Summer_Time" target="_blank" rel="noopener">CEST</a> vilket lyckligtvis är ekvivalent med Svensk tid.</p><p>I samband med att programvaran för certifieringstestet installeras så läggs en genväg med namn <em>PVproctor</em> på skrivbordet. Observera att du inte ska använda denna. Om du startar programmet via genvägen så blir du promptad att mata in en niosiffrig accesskod. Kruxet är att du inte har tillgång till någon sådan. För att starta testet är man tvungen att göra det via hemsidan.</p><h2 id="Hur-gor-jag-om-nagot-inte-fungerar"><a href="#Hur-gor-jag-om-nagot-inte-fungerar" class="headerlink" title="Hur gör jag om något inte fungerar?"></a>Hur gör jag om något inte fungerar?</h2><p>Du kan ringa och prata med <a href="http://www.pearsonvue.com/microsoft/contact/" target="_blank" rel="noopener">supporten på Pearson VUE</a>. För oss i Sverige kontaktar man dem via <a href="tel:020798690">020-798690</a>.</p>]]></content>
    
    <summary type="html">
    
      För att slippa åka till ett testcenter kan man göra certifieringstest för Microsoft från sin egen dator. Att ha en webbkamera med kort fokusavstånd är bra!
    
    </summary>
    
    
    
  </entry>
  
</feed>
